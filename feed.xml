<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ahmed Akakzia</title>
        <description>Welcome to my professional and personal website.</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Wed, 26 Oct 2022 18:36:55 +0200</pubDate>
        <lastBuildDate>Wed, 26 Oct 2022 18:36:55 +0200</lastBuildDate>
        <generator>Jekyll v4.0.1</generator>
        
            <item>
                <title>When Graph Neural Networks Meet Reinforcement Learning</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Reinforcement Learning (RL) agents should be able to efficiently generalize to novel situations and transfer their learned skills. Without these properties, such agents would always have to learn from scratch, even though they have already mastered primitive skills that could potentially be leveraged to acquire more complex ones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Combining primitive skills and building upon them to solve harder tasks is a key challenge within artificial intelligence. In the context of &lt;em&gt;goal-conditioned agents&lt;/em&gt;, transfer and adaptibility seem to depend on two key features: &lt;em&gt;the goal space design&lt;/em&gt;, and &lt;em&gt;the policy architecture&lt;/em&gt;. On the one hand, the goal representation—whether it is learned or predefined—should encapsulate an adequate structure that defines a specific topology in the goal space.&lt;/p&gt;

&lt;p&gt;On the other hand, since the behavior of artificial agents does not only depend on how they represent their goals, but also on how they take actions, we investigate &lt;em&gt;Graph Neural Networks&lt;/em&gt; (GNNs) as technical tools to model policies in autotelic agents. This choice is also motivated by developmental approaches, as research in psychology shows that humans perceive their world in a structured fashion &lt;a class=&quot;citation&quot; href=&quot;#winston1970learning&quot;&gt; [1,2,3,4,5,6,7,8,9]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This blog post is organized as follows. First, we start by introducing GNNs as technical tools to endow artificial agents with relational inductive biases. Then, we present an overview on the use of GNNs in the field of RL. Finally, we highlight several limitations of such combination.&lt;/p&gt;

&lt;h2 id=&quot;graph-neural-networks&quot;&gt;Graph Neural Networks&lt;/h2&gt;

&lt;p&gt;Recently, deep learning methods have been used to solve a significant amount of problems in different domains. Ranging from image classification &lt;a class=&quot;citation&quot; href=&quot;#redmon2016you&quot;&gt; [10,11]&lt;/a&gt; and video processing &lt;a class=&quot;citation&quot; href=&quot;#zhang2016deep&quot;&gt; [12]&lt;/a&gt; to speech recognition &lt;a class=&quot;citation&quot; href=&quot;#hinton2012deep&quot;&gt; [13]&lt;/a&gt; and neural machine translation &lt;a class=&quot;citation&quot; href=&quot;#luong2015effective&quot;&gt; [14,15]&lt;/a&gt;, these methods use parameterized neural networks as building blocks. Consequently, such methods are usually end-to-end, requiring few to no assumptions. They feed their networks with raw streams of data which are usually represented in the Euclidean Space. However, many applications rather represent data in non-Euclidean domains and use graphs with complex relationships and inter-dependencies. Standard usage of deep learning techniques usually struggle with this type of unstructured representations.&lt;/p&gt;

&lt;p&gt;Interestingly, research has been interested in leveraging graph-based information using neural networks. Namely, &lt;em&gt;Graph Neural Networks&lt;/em&gt; (GNNs) were proposed as computational frameworks that handle unstructured data using neural networks that they share between nodes and edges &lt;a class=&quot;citation&quot; href=&quot;#wang2016learning&quot;&gt; [16,7,17,18,19,20,8,21,22,23,24,25,26,27]&lt;/a&gt;. Although these methods are all based on the same idea, they use different techniques depending on how they handle computations within their GNNs’ definition. There exist several surveys that propose different taxonomies for GNNs-based methods &lt;a class=&quot;citation&quot; href=&quot;#bronstein2017geometric&quot;&gt; [28,29,8,30,31]&lt;/a&gt;. In this section, rather than presenting an exhaustive survey of GNNs, our goal is to define the building blocks including definitions and computational schemes. Besides, we focus on applications in RL and present a short overview of standard methods.&lt;/p&gt;

&lt;h3 id=&quot;relational-inductive-bias-with-graph-neural-networks&quot;&gt;Relational Inductive Bias with Graph Neural Networks&lt;/h3&gt;

&lt;p&gt;First, we propose a definition for the central component of GNNs: the graph.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graph.&lt;/strong&gt; A graph is a mathematical structure used to model &lt;em&gt;pairwise relations&lt;/em&gt; between &lt;em&gt;objects&lt;/em&gt;. More formally, we denote a graph by an ordered pair \(G=(V, E)\), where \(V\) is the set of vertices or nodes—the objects—and \(E\) is the set of edges—the pairwise relations. We denote a single node by \(v_i \in V\), and an edge traveling from node \(v_i\) to node \(v_j\) as \(e_{ij} \in E\). We also define the neighborhood of a node \(v_i\) to be the set of nodes to which \(v_i\) is connected by an edge. Formally, this set is defined as&lt;/p&gt;

\[\mathcal{N}(v_i) = \{v_j \in V~|~e_{ij} \in E\}.\]

&lt;p&gt;Finally, we consider some global features which characterize the whole graph, and we denote them by \(u\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Undirected and Directed Graphs.&lt;/strong&gt; The definition above suggests that the edges of a graph \(G\) are inherently directed from a &lt;em&gt;source&lt;/em&gt; node to a &lt;em&gt;recipient&lt;/em&gt; node. In some special scenarios, a graph can be &lt;em&gt;undirected&lt;/em&gt;: that is, \(e_{ij} = e_{ji}\) for each pair of nodes \(v_i\) and \(v_j\). In this case, the relation between nodes is said to be &lt;em&gt;symmetric&lt;/em&gt;. If the edges are distinguished from their inverted counterparts (\(e_{ij} \neq e_{ji}\)), then the graph is said to be &lt;em&gt;directed&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;graph-input&quot;&gt;Graph Input&lt;/h4&gt;

&lt;p&gt;The input of a graph corresponds to the parsed input features of all its nodes, all its edges and some other global features characterizing the whole system. Active lines of research that are orthogonal to our work are exploring methods that enable the extraction of such parsed features from raw sensory data &lt;a class=&quot;citation&quot; href=&quot;#watters2017visual&quot;&gt; [32,33,34,35]&lt;/a&gt;. To simplify our study, we suppose the existence of a &lt;em&gt;predefined feature extractor&lt;/em&gt; that automatically generates input values for each node and edge. For simplicity, we respectively denote the input features of node \(i\), edge \(i \rightarrow j\) and global features by \(v_i\), \(e_{ij}\) and \(u\).&lt;/p&gt;

&lt;h4 id=&quot;graph-output&quot;&gt;Graph Output&lt;/h4&gt;

&lt;p&gt;Depending on the graph structure and the task at hand, the output of the graph can focus on different graph levels. If the functions used to produce this output are modeled by neural networks, then we speak about GNNs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Node-level.&lt;/strong&gt; This level focuses on the nodes of the graph. In this scenario, input features including node, edge and global features are used to produce a new embedding for each node. This can be used to perform regression and classification at the level of nodes and learn about the physical dynamics of each object &lt;a class=&quot;citation&quot; href=&quot;#battaglia2016interaction&quot;&gt; [7,36,22,20]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edge-level.&lt;/strong&gt; This level focuses on the edges of the graph. The output of the computational scheme in this case are the updated features of each node after propagating the information between all the nodes. For instance, it can be used to make decisions about interactions among the different objects &lt;a class=&quot;citation&quot; href=&quot;#kipf2018neural&quot;&gt; [35,37]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graph-level.&lt;/strong&gt; This level focuses on the entire graph. The output corresponds to a global embedding computed after propagating the information between all nodes of the graph. It can be used by embodied agents to produce actions in multi-object scenarios &lt;a class=&quot;citation&quot; href=&quot;#akakzia2021grounding&quot;&gt; [26,27]&lt;/a&gt;, to answer questions about a visual scene &lt;a class=&quot;citation&quot; href=&quot;#santoro2017simple&quot;&gt; [17]&lt;/a&gt; or to extract the global properties molecules in chemistry &lt;a class=&quot;citation&quot; href=&quot;#gilmer2017neural&quot;&gt; [38]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;graph-computation&quot;&gt;Graph Computation&lt;/h4&gt;

&lt;p&gt;So far, we have formally defined graphs and distinguished three types of attention-levels which define their output. Thereafter, we explain how exactly the computation of this output is conducted. The computational scheme within GNNs involves two main properties. First, it is based on &lt;em&gt;shared&lt;/em&gt; neural networks which are used to compute the updated features of all the nodes and edges. Second, it uses &lt;em&gt;aggregation functions&lt;/em&gt; that pool these features in order to produce the output. These two properties provide GNNs with good combinatorial generalization capabilities. In fact, not only it enables good transfer between different nodes and edges (based on the shared networks), but also it leverages permutation invariance (based on the aggregation scheme).&lt;/p&gt;

&lt;p&gt;We denote the shared neural networks between the nodes by \(NN_{nodes}\), the shared neural networks between edges by \(NN_{edges}\), and the readout neural network that produces the global output of the GNN by \(NN_{readout}\). Besides, we focus on &lt;em&gt;graph-level output&lt;/em&gt;. The full computational scheme is based on three steps: the &lt;em&gt;edge updates&lt;/em&gt;, the &lt;em&gt;node updates&lt;/em&gt; and the &lt;em&gt;graph readout&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The edge update step.&lt;/strong&gt; The edge update step consists in using the input features involving each edge \(i \rightarrow j\) to compute its updated features, which we note \(e'_{ij}\). More precisely, we consider the global input feature \(u\), the input features of the source node \(v_i\) and the input features of the recipient node \(v_j\). We use the shared network \(NN_{edges}\) to compute the updated features of all the edges. Formally, the updated features \(e'_{ij}\) of the edge \(i \rightarrow j\) are computed as follows:&lt;/p&gt;

\[e'_{ij}~=~NN_{edges}(v_i, v_j, e_{ij}, u).\]

&lt;p&gt;&lt;strong&gt;The node update step.&lt;/strong&gt; The node update step aims at computing the updated features of all the nodes. We note \(v'_{i}\) these updated features for node \(i\). To do so, the input features of the underlying node, the global features as well as the aggregation of the updated features of the incoming edges to \(i\) are considered. The incoming edges to \(i\) correspond to edges whose source nodes are necessarily in the neighborhood of \(i\), \(\mathcal{N}(i)\). The shared network \(NN_{nodes}\) is used in this computation. Formally, the updated features \(v'_{i}\) of the node \(i\) are obtained as follows:&lt;/p&gt;

\[v'_{i}~=~NN_{nodes}(v_i, Agg_{i \in \mathcal{N}(i)}(e'_{ij}), u).\]

&lt;p&gt;&lt;strong&gt;The graph readout step.&lt;/strong&gt; The graph readout step computes the global output of the graph. This quantity is obtained by aggregating all the updated features of the nodes within the graph. It uses the readout neural network \(NN_{readout}\). Formally, the output \(o\) of the GNN is computed as follows:&lt;/p&gt;

\[o~=~NN_{readout}(Agg_{i \in graph}(v'_{i})).\]

&lt;p&gt;The computational steps we described above can be used in some other order. For example, one can first perform the node update using the input features of edges, then perform the edge updates using the updated nodes features. This choice usually depends on the domain and task at hand. Besides, our descriptions above are categorized within the family of &lt;em&gt;convolutional GNNs&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#bruna2013spectral&quot;&gt; [39,40,41,42,43,38,27]&lt;/a&gt;, which generalize the operation of convolution from grid data to graph data by pooling features of neighbors when updating each node. There exist other categories of GNNs, such as &lt;em&gt;graph auto-encoders&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#cao2016deep&quot;&gt; [44,45,46,47,34]&lt;/a&gt;, &lt;em&gt;spatio-temporal GNNs&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#yu2017spatio&quot;&gt; [48,49,50,51]&lt;/a&gt; and &lt;em&gt;recurrent GNNs&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#scarselli2005graph&quot;&gt; [52,53,54,55]&lt;/a&gt;. Finally, the aggregation module used to perform node-wise pooling can be either some predefined permutation-invariant function such as sum, max or mean, or a more sophisticated self-attention-based function that learns attention weights for each node &lt;a class=&quot;citation&quot; href=&quot;#velivckovic2017graph&quot;&gt; [56]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;overview-on-graph-neural-networks-in-rl&quot;&gt;Overview on Graph Neural Networks in RL&lt;/h3&gt;

&lt;p&gt;Recently, Graph Neural Networks have been widely used in Reinforcement Learning. In fact, they promote sample efficiency, especially in multi-object manipulation domains, where object invariance becomes crucial for generalization. In this paragraph, we introduce an overview over recent works in RL using GNNs. We divide the works in two categories: GNNs used for &lt;em&gt;model-based&lt;/em&gt; RL and for &lt;em&gt;model-free&lt;/em&gt; RL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model-based Reinforcement Learning.&lt;/strong&gt; The idea of using GNNs in model-based reinforcement learning settings mainly amounts to representing the perceived world of the artificial agents with graphs. Recent papers have been using GNNs to learn prediction models by construction graph representations using the bodies and joints of the agents &lt;a class=&quot;citation&quot; href=&quot;#wang2016learning&quot;&gt; [16,19,20]&lt;/a&gt;. This approach is shown to be successful in prediction, system identification and planning. However, these approaches struggle when the structure of the components and joints of the agent are different. For example, they work better on the Swimmer environment than HalfCheetah, since the latter contains more joints corresponding to different components (back leg, front leg, head …). Other approaches use Interaction Networks &lt;a class=&quot;citation&quot; href=&quot;#battaglia2016interaction&quot;&gt; [7]&lt;/a&gt;, which are a particular type of GNNs to implement transition models of the environment which they later use for imagination-based optimization &lt;a class=&quot;citation&quot; href=&quot;#hamrick2017metacontrol&quot;&gt; [19]&lt;/a&gt; or planning from scratch &lt;a class=&quot;citation&quot; href=&quot;#wang2016learning&quot;&gt; [16]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model-free Reinforcement Learning.&lt;/strong&gt; GNNs are also used in model-free reinforcement learning to model the policy and / or the value function &lt;a class=&quot;citation&quot; href=&quot;#wang2018nervenet&quot;&gt; [22,21,23,24,25,26]&lt;/a&gt;. On the one hand, like the model-based setting, some approaches use them to represent the agent’s body and joints as a graph where the different components interact with each other to produce an action &lt;a class=&quot;citation&quot; href=&quot;#wang2018nervenet&quot;&gt; [22]&lt;/a&gt;. On the other hand, other approaches use it to represent the world in term of separate entities and attempt to capture the relational features between them &lt;a class=&quot;citation&quot; href=&quot;#zambaldi2018relational&quot;&gt; [21,23,24,25,26]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;

&lt;p&gt;In spite of their generalization capacities provided by their permutation invariance, GNNs still show some limitations to solve some classes of problems such as discriminating between certain non-isomorphic graphs &lt;a class=&quot;citation&quot; href=&quot;#kondor2018generalization&quot;&gt; [57]&lt;/a&gt;. Moreover, notions like recursion, control flow and conditional iteration are not straightforward to represent with graphs, and might require some domain-specific tweaks (for example, in interpreting abstract syntax trees). In fact, symbolic  programs using probabilistic models are shown to work better on these classes of problems &lt;a class=&quot;citation&quot; href=&quot;#tenenbaum2011grow&quot;&gt; [6,58,59]&lt;/a&gt;. But more importantly, a more pressing question is about the origin of the graph networks that most of the methods work on. In fact, most approaches that use GNNs use graphs with predefined entities corresponding to structured objects. Removing this assumption, it is still unclear how to convert sensory data into more structured graph-like representations. Some lines of active research are exploring these issues &lt;a class=&quot;citation&quot; href=&quot;#watters2017visual&quot;&gt; [32,33,34,35]&lt;/a&gt;.&lt;/p&gt;

&lt;!--- References --&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;winston1970learning&quot;&gt;[1]P. H. Winston, &lt;i&gt;Learning Structural Descriptions from Examples&lt;/i&gt;, None (1970).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;palmer1975visual&quot;&gt;[2]S. E. Palmer, &lt;i&gt;Visual Perception and World Knowledge: Notes on a Model of Sensory-Cognitive Interaction&lt;/i&gt;, Explorations in Cognition 279 (1975).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;navon1977forest&quot;&gt;[3]D. Navon, &lt;i&gt;Forest before Trees: The Precedence of Global Features in Visual Perception&lt;/i&gt;, Cognitive Psychology &lt;b&gt;9&lt;/b&gt;, 353 (1977).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;markman1989categorization&quot;&gt;[4]E. M. Markman, &lt;i&gt;Categorization and Naming in Children: Problems of Induction&lt;/i&gt; (mit Press, 1989).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kemp2008discovery&quot;&gt;[5]C. Kemp and J. B. Tenenbaum, &lt;i&gt;The Discovery of Structural Form&lt;/i&gt;, Proceedings of the National Academy of Sciences &lt;b&gt;105&lt;/b&gt;, 10687 (2008).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tenenbaum2011grow&quot;&gt;[6]J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. Goodman, &lt;i&gt;How to Grow a Mind: Statistics, Structure, and Abstraction&lt;/i&gt;, Science &lt;b&gt;331&lt;/b&gt;, 1279 (2011).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;battaglia2016interaction&quot;&gt;[7]P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, and others, &lt;i&gt;Interaction Networks for Learning about Objects, Relations and Physics&lt;/i&gt;, Advances in Neural Information Processing Systems &lt;b&gt;29&lt;/b&gt;, (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;battaglia2018relational&quot;&gt;[8]P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, and others, &lt;i&gt;Relational Inductive Biases, Deep Learning, and Graph Networks&lt;/i&gt;, ArXiv Preprint ArXiv:1806.01261 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;godfrey2021theory&quot;&gt;[9]P. Godfrey-Smith, &lt;i&gt;Theory and Reality&lt;/i&gt;, in &lt;i&gt;Theory and Reality&lt;/i&gt; (University of Chicago Press, 2021).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;redmon2016you&quot;&gt;[10]J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, &lt;i&gt;You Only Look Once: Unified, Real-Time Object Detection&lt;/i&gt;, in &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (2016), pp. 779–788.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ren2015faster&quot;&gt;[11]S. Ren, K. He, R. Girshick, and J. Sun, &lt;i&gt;Faster r-Cnn: Towards Real-Time Object Detection with Region Proposal Networks&lt;/i&gt;, Advances in Neural Information Processing Systems &lt;b&gt;28&lt;/b&gt;, (2015).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zhang2016deep&quot;&gt;[12]W. Zhang, L. Xu, Z. Li, Q. Lu, and Y. Liu, &lt;i&gt;A Deep-Intelligence Framework for Online Video Processing&lt;/i&gt;, IEEE Software &lt;b&gt;33&lt;/b&gt;, 44 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hinton2012deep&quot;&gt;[13]G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-rahman Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and others, &lt;i&gt;Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups&lt;/i&gt;, IEEE Signal Processing Magazine &lt;b&gt;29&lt;/b&gt;, 82 (2012).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;luong2015effective&quot;&gt;[14]M.-T. Luong, H. Pham, and C. D. Manning, &lt;i&gt;Effective Approaches to Attention-Based Neural Machine Translation&lt;/i&gt;, ArXiv Preprint ArXiv:1508.04025 (2015).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2017sequence&quot;&gt;[15]S. Wu, D. Zhang, N. Yang, M. Li, and M. Zhou, &lt;i&gt;Sequence-to-Dependency Neural Machine Translation&lt;/i&gt;, in &lt;i&gt;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/i&gt; (2017), pp. 698–707.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2016learning&quot;&gt;[16]J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, &lt;i&gt;Learning to Reinforcement Learn&lt;/i&gt;, ArXiv Preprint ArXiv:1611.05763 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;santoro2017simple&quot;&gt;[17]A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap, &lt;i&gt;A Simple Neural Network Module for Relational Reasoning&lt;/i&gt;, Advances in Neural Information Processing Systems &lt;b&gt;30&lt;/b&gt;, (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zaheer2017deep&quot;&gt;[18]M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola, &lt;i&gt;Deep Sets&lt;/i&gt;, in &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; (2017), pp. 3391–3401.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hamrick2017metacontrol&quot;&gt;[19]J. B. Hamrick, A. J. Ballard, R. Pascanu, O. Vinyals, N. Heess, and P. W. Battaglia, &lt;i&gt;Metacontrol for Adaptive Imagination-Based Optimization&lt;/i&gt;, ArXiv Preprint ArXiv:1705.02670 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sanchez2018graph&quot;&gt;[20]A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia, &lt;i&gt;Graph Networks as Learnable Physics Engines for Inference and Control&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2018), pp. 4470–4479.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zambaldi2018relational&quot;&gt;[21]V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart, and others, &lt;i&gt;Relational Deep Reinforcement Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1806.01830 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2018nervenet&quot;&gt;[22]T. Wang, R. Liao, J. Ba, and S. Fidler, &lt;i&gt;NerveNet: Learning Structured Policy with Graph Neural Networks&lt;/i&gt;, in &lt;i&gt;International Conference on Learning Representations&lt;/i&gt; (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bapst2019structured&quot;&gt;[23]V. Bapst, A. Sanchez-Gonzalez, C. Doersch, K. Stachenfeld, P. Kohli, P. Battaglia, and J. B. Hamrick, &lt;i&gt;Structured Agents for Physical Construction&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2019), pp. 464–474.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;li2019towards&quot;&gt;[24]R. Li, A. Jabri, T. Darrell, and P. Agrawal, &lt;i&gt;Towards Practical Multi-Object Manipulation Using Relational Reinforcement Learning&lt;/i&gt;, ArXiv Preprint &lt;b&gt;abs/1912.11032&lt;/b&gt;, (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;colas2020language&quot;&gt;[25]C. Colas, T. Karch, N. Lair, J.-M. Dussoux, C. Moulin-Frier, P. F. Dominey, and P.-Y. Oudeyer, &lt;i&gt;Language as a Cognitive Tool to Imagine Goals in Curiosity Driven
Exploration&lt;/i&gt;, in &lt;i&gt;Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, Virtual&lt;/i&gt;, edited by H. Larochelle, M. A. Ranzato, R. Hadsell, M.-F. Balcan, and H.-T. Lin (2020).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;akakzia2021grounding&quot;&gt;[26]A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud, &lt;i&gt;Grounding Language to Autonomously-Acquired Skills via Goal Generation&lt;/i&gt;, in &lt;i&gt;9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021&lt;/i&gt; (OpenReview.net, 2021).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;akakzia2022learning&quot;&gt;[27]A. Akakzia and O. Sigaud, &lt;i&gt;Learning Object-Centered Autotelic Behaviors with Graph Neural Networks&lt;/i&gt;, ArXiv Preprint ArXiv:2204.05141 (2022).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bronstein2017geometric&quot;&gt;[28]M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, &lt;i&gt;Geometric Deep Learning: Going beyond Euclidean Data&lt;/i&gt;, IEEE Signal Processing Magazine &lt;b&gt;34&lt;/b&gt;, 18 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hamilton2017representation&quot;&gt;[29]W. L. Hamilton, R. Ying, and J. Leskovec, &lt;i&gt;Representation Learning on Graphs: Methods and Applications&lt;/i&gt;, ArXiv Preprint ArXiv:1709.05584 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lee2018graph&quot;&gt;[30]J. B. Lee, R. Rossi, and X. Kong, &lt;i&gt;Graph Classification Using Structural Attention&lt;/i&gt;, in &lt;i&gt;Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining&lt;/i&gt; (2018), pp. 1666–1674.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2020comprehensive&quot;&gt;[31]Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, &lt;i&gt;A Comprehensive Survey on Graph Neural Networks&lt;/i&gt;, IEEE Transactions on Neural Networks and Learning Systems &lt;b&gt;32&lt;/b&gt;, 4 (2020).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;watters2017visual&quot;&gt;[32]N. Watters, D. Zoran, T. Weber, P. Battaglia, R. Pascanu, and A. Tacchetti, &lt;i&gt;Visual Interaction Networks: Learning a Physics Simulator from Video&lt;/i&gt;, Advances in Neural Information Processing Systems &lt;b&gt;30&lt;/b&gt;, (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;van2018relational&quot;&gt;[33]S. Van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber, &lt;i&gt;Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and Their Interactions&lt;/i&gt;, ArXiv Preprint ArXiv:1802.10353 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;li2018learning&quot;&gt;[34]Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, &lt;i&gt;Learning Deep Generative Models of Graphs&lt;/i&gt;, ArXiv Preprint ArXiv:1803.03324 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipf2018neural&quot;&gt;[35]T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel, &lt;i&gt;Neural Relational Inference for Interacting Systems&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2018), pp. 2688–2697.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chang2016compositional&quot;&gt;[36]M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, &lt;i&gt;A Compositional Object-Based Approach to Learning Physical Dynamics&lt;/i&gt;, ArXiv Preprint ArXiv:1612.00341 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hamrick2018relational&quot;&gt;[37]J. B. Hamrick, K. R. Allen, V. Bapst, T. Zhu, K. R. McKee, J. B. Tenenbaum, and P. W. Battaglia, &lt;i&gt;Relational Inductive Bias for Physical Construction in Humans and Machines&lt;/i&gt;, ArXiv Preprint &lt;b&gt;abs/1806.01203&lt;/b&gt;, (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gilmer2017neural&quot;&gt;[38]J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, &lt;i&gt;Neural Message Passing for Quantum Chemistry&lt;/i&gt;, ArXiv Preprint ArXiv:1704.01212 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bruna2013spectral&quot;&gt;[39]J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, &lt;i&gt;Spectral Networks and Locally Connected Networks on Graphs&lt;/i&gt;, ArXiv Preprint ArXiv:1312.6203 (2013).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;henaff2015deep&quot;&gt;[40]M. Henaff, J. Bruna, and Y. LeCun, &lt;i&gt;Deep Convolutional Networks on Graph-Structured Data&lt;/i&gt;, ArXiv Preprint ArXiv:1506.05163 (2015).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;defferrard2016convolutional&quot;&gt;[41]M. Defferrard, X. Bresson, and P. Vandergheynst, &lt;i&gt;Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering&lt;/i&gt;, Advances in Neural Information Processing Systems &lt;b&gt;29&lt;/b&gt;, (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipf2016semi&quot;&gt;[42]T. N. Kipf and M. Welling, &lt;i&gt;Semi-Supervised Classification with Graph Convolutional Networks&lt;/i&gt;, ArXiv Preprint ArXiv:1609.02907 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;levie2018cayleynets&quot;&gt;[43]R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, &lt;i&gt;Cayleynets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters&lt;/i&gt;, IEEE Transactions on Signal Processing &lt;b&gt;67&lt;/b&gt;, 97 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cao2016deep&quot;&gt;[44]S. Cao, W. Lu, and Q. Xu, &lt;i&gt;Deep Neural Networks for Learning Graph Representations&lt;/i&gt;, in &lt;i&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/i&gt;, Vol. 30 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2016structural&quot;&gt;[45]D. Wang, P. Cui, and W. Zhu, &lt;i&gt;Structural Deep Network Embedding&lt;/i&gt;, in &lt;i&gt;Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining&lt;/i&gt; (2016), pp. 1225–1234.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipf2016variational&quot;&gt;[46]T. N. Kipf and M. Welling, &lt;i&gt;Variational Graph Auto-Encoders&lt;/i&gt;, ArXiv Preprint ArXiv:1611.07308 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pan2018adversarially&quot;&gt;[47]S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, &lt;i&gt;Adversarially Regularized Graph Autoencoder for Graph Embedding&lt;/i&gt;, ArXiv Preprint ArXiv:1802.04407 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yu2017spatio&quot;&gt;[48]B. Yu, H. Yin, and Z. Zhu, &lt;i&gt;Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting&lt;/i&gt;, ArXiv Preprint ArXiv:1709.04875 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;li2017diffusion&quot;&gt;[49]Y. Li, R. Yu, C. Shahabi, and Y. Liu, &lt;i&gt;Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting&lt;/i&gt;, ArXiv Preprint ArXiv:1707.01926 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;seo2018structured&quot;&gt;[50]Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, &lt;i&gt;Structured Sequence Modeling with Graph Convolutional Recurrent Networks&lt;/i&gt;, in &lt;i&gt;International Conference on Neural Information Processing&lt;/i&gt; (Springer, 2018), pp. 362–373.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;guo2019attention&quot;&gt;[51]S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, &lt;i&gt;Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting&lt;/i&gt;, in &lt;i&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/i&gt;, Vol. 33 (2019), pp. 922–929.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;scarselli2005graph&quot;&gt;[52]F. Scarselli, S. L. Yong, M. Gori, M. Hagenbuchner, A. C. Tsoi, and M. Maggini, &lt;i&gt;Graph Neural Networks for Ranking Web Pages&lt;/i&gt;, in &lt;i&gt;The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)&lt;/i&gt; (IEEE, 2005), pp. 666–672.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gallicchio2010graph&quot;&gt;[53]C. Gallicchio and A. Micheli, &lt;i&gt;Graph Echo State Networks&lt;/i&gt;, in &lt;i&gt;The 2010 International Joint Conference on Neural Networks (IJCNN)&lt;/i&gt; (IEEE, 2010), pp. 1–8.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;li2015gated&quot;&gt;[54]Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, &lt;i&gt;Gated Graph Sequence Neural Networks&lt;/i&gt;, ArXiv Preprint ArXiv:1511.05493 (2015).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dai2018learning&quot;&gt;[55]H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, &lt;i&gt;Learning Steady-States of Iterative Algorithms over Graphs&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2018), pp. 1106–1114.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;velivckovic2017graph&quot;&gt;[56]P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, &lt;i&gt;Graph Attention Networks&lt;/i&gt;, ArXiv Preprint ArXiv:1710.10903 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kondor2018generalization&quot;&gt;[57]R. Kondor and S. Trivedi, &lt;i&gt;On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2018), pp. 2747–2755.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goodman2014concepts&quot;&gt;[58]N. D. Goodman, J. B. Tenenbaum, and T. Gerstenberg, Concepts in a Probabilistic Language of Thought, Center for Brains, Minds and Machines (CBMM), 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lake2015human&quot;&gt;[59]B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, &lt;i&gt;Human-Level Concept Learning through Probabilistic Program Induction&lt;/i&gt;, Science &lt;b&gt;350&lt;/b&gt;, 1332 (2015).&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
                <pubDate>Sun, 25 Sep 2022 00:00:00 +0200</pubDate>
                <link>http://localhost:4000/graph-neural-networks</link>
                <guid isPermaLink="true">http://localhost:4000/graph-neural-networks</guid>
                
                
            </item>
        
            <item>
                <title>Goal Exploration Processes</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Autotelic agents—agents that are intrinsically motivated to represent, generate and pursue their own goals—aim at growing their repertoire of skills. This implies that they need not only to discover as many goals as possible, but also to learn to achieve each of these goals. When these agents evolve in environments where they have no clue about which goals they can physically reach in the first place, it becomes challenging to handle the &lt;em&gt;exploration-exploitation&lt;/em&gt; dilemma.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;The standard Reinforcement Learning (RL) setup is not suitable for training artificial agents to achieve a set of different goals, since there is usually a unique mapping between a goal and a reward signal. A straightforward way to circumvent this issue is to define &lt;em&gt;goal experts&lt;/em&gt; modules.  This implies that an embodied agent would have a set of policies equal to the number of potentially learnable goals. Whenever the agent attempts to reach a particular goal, it selects actions according to the policy that corresponds to this goal. These methods defined the first attempts to solve multi-goal problems &lt;a class=&quot;citation&quot; href=&quot;#kaelbling1993learning&quot;&gt; [1,2]&lt;/a&gt;, some of which used modular representations of the state space &lt;a class=&quot;citation&quot; href=&quot;#forestier2016modular&quot;&gt; [3]&lt;/a&gt;. Unfortunately, all these methods present two main drawbacks. First, they all require knowing the number of goals beforehand in order to define the number of policies to be trained. Second, they do not leverage generalization and transfer between goals, since all the policies are by definition independent from one another.&lt;/p&gt;

&lt;p&gt;Recently, with the promising results leveraged by neural networks as universal function approximators, a new framework where a single policy could learn to achieve multiple goals has been developed. This defines the sub-family of &lt;em&gt;Goal-Conditioned Reinforcement Learning&lt;/em&gt; (GCRL), which originated from results on universal value function approximators &lt;a class=&quot;citation&quot; href=&quot;#schaul2015universal&quot;&gt; [4]&lt;/a&gt;. The main principle is simply to condition the agent’s policy not only on observations or states, but also on embeddings of the goals to be achieved. Instead of having one policy for each goal, these methods have a single &lt;em&gt;contextual&lt;/em&gt; policy, where the context defines the goal &lt;a class=&quot;citation&quot; href=&quot;#andrychowicz2017hindsight&quot;&gt; [5,6,7]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;formalizing-multi-goal-reinforcement-learning-problems&quot;&gt;Formalizing Multi-Goal Reinforcement Learning Problems&lt;/h2&gt;

&lt;p&gt;We propose to formalize multi-goal reinforcement learning problems. While standard RL uses a single &lt;em&gt;Markov Decision Process&lt;/em&gt; (MDP) and requires the agent to finish one specific task defined by the reward function, GCRL focuses on a more general and more complex scenario where agents can fulfill multiple tasks simultaneously. To tackle such a challenge, we introduce a goal space \(\mathcal{G}~=~Z_{\mathcal{G}} \times R_{\mathcal{G}}\), where \(Z_{\mathcal{G}}\) denotes the space of goal embeddings and \(R_{\mathcal{G}}\) is the space of the corresponding reward functions. We also introduce a tractable mapping function \(\phi~:~\mathcal{S} \rightarrow Z_{\mathcal{G}}\) that maps the state to a specific goal embedding. The term &lt;em&gt;goal&lt;/em&gt; should be differentiated from the term &lt;em&gt;task&lt;/em&gt;, which refers to a particular MDP instance. Next, we need to differentiate the notions of &lt;em&gt;desired goal&lt;/em&gt; and &lt;em&gt;achieved goal&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Achieved Goal:&lt;/strong&gt; An achieved goal defines the outcome of the actions conducted by the agent during a rollout episode. More specifically, it is the output of the mapping function applied at time step \(t\) on the current state of the agent: \(\phi(s_t)\). We denote by \(p^a_\mathcal{G}\) the distribution of achieved goals. Note that these goals are exactly the goals &lt;em&gt;discovered&lt;/em&gt; by the agent in play.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Desired Goal:&lt;/strong&gt; A desired goal defines the task that the agent attempts to solve. It can be either provided externally (by a simulator or an external instructing program) or generated intrinsically. We denote by \(p^d_{\mathcal{G}}\) the distribution of desired goals. This distribution is predefined when the agent receives goals from its external world, and corresponds to the distribution of achieved goals if the agent is intrinsically motivated.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on these definitions, we extend RL problems to handle multiple goals by defining an augmented MDP \(\mathcal{M} = \{\mathcal{S}, \mathcal{A}, \mathcal{T}, \rho_0, \mathcal{G}, p^d_{\mathcal{G}}, \phi\}\). Consequently, the objective of \gcrl is to learn a goal-conditioned policy \(\pi~:~\mathcal{S} \times \mathcal{A} \times \mathcal{G} \rightarrow [0, 1]\) that maximizes the expectation of the cumulative reward over the distribution of desired goals:&lt;/p&gt;

\[\pi^* = \textrm{arg}\max_{\pi} ~ \mathbb{E}_{\substack{g\sim p^d_{\mathcal{G}} \textrm{, } s_0\sim \rho_0 \\ a_t\sim \pi(.~|~s_t, g) \\ s_{t+1}\sim \mathcal{T}(s_t,a_t)}} \Big[\sum_t \gamma^t R_{\mathcal{G}} (\phi(s_{t+1})~|~z_g) \Big].\]

&lt;h2 id=&quot;goal-exploration-processes&quot;&gt;Goal Exploration Processes&lt;/h2&gt;

&lt;p&gt;In multi-goal setups, the objective of goal-conditioned artificial agents is to simultaneously learn as many goals as possible. In other words, the training of such agents should in principle yield optimal goal-conditioned policies that maximize the coverage of the goal space. This coverage is usually defined with reference to the distribution of desired goals. Hence, agents should be able to efficiently explore their behavioral goal space in order to match the widest possible distribution of desired goals. Goal Exploration Processes (GEPs) are a family of frameworks for exploring multiple goals. For any environment—which can be defined by a state space \(\mathcal{S}\), an action space \(\mathcal{A}\) and a transition distribution \(\mathcal{T}\) that determines the next state given a current state and an action—a GEP essentially aims at maximizing its behavioral diversity by exploring the maximum number of goals. We consider goals here as pairs composed of a fitness function and a goal embedding, where the latter is the result of projecting the state space on a predefined or learned goal space \(\mathcal{G}\) using a surjective function: each goal is mapped to at least one state.&lt;/p&gt;

&lt;p&gt;GEPs were first defined in the context of intrinsically motivated population based agents &lt;a class=&quot;citation&quot; href=&quot;#forestier2017intrinsically&quot;&gt; [8]&lt;/a&gt;. In this section, we present GEPs as a general framework regardless of the underlying motivations (which can either be external or internal). First, we start from the policy search view on GEPs to derive a policy gradient perspective for goal-conditioned RL agents (See Figure 1 for an illustration). Then, depending on the source of motivations, we present the sub-families: Externally Motivated and Internally Motivated GEPs.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/perspectives.jpg&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.1-Illustration of the two stages leveraged by the Goal Exploration Processes (GEPs), as seen from the policy search perspective (left) and the goal-conditioned \rlearning perspective (right).
&lt;/p&gt;

&lt;h3 id=&quot;geps-policy-search-perspective&quot;&gt;GEPs: Policy Search Perspective&lt;/h3&gt;

&lt;p&gt;From the policy search point of view, GEPs explore multiple goals starting from an initial population of policy parameters. The process leverages two phases: a first phase called the &lt;em&gt;bootstrapping phase&lt;/em&gt;, which is conducted once, and a second phase called the &lt;em&gt;search loop&lt;/em&gt;, which is repeated until convergence. Both phases require an &lt;em&gt;outcome extractor&lt;/em&gt;, which is a predefined deterministic function that takes as input the policy parameters and outputs the outcome of applying that particular policy in the environment.&lt;/p&gt;

&lt;p&gt;Concerning the bootstrapping phase, \(N\) sets of policy parameters are randomly sampled from \(\Theta\). Each one of the sampled policies is fed the outcome extractor to observe the corresponding outcome, which lays in an outcome space \(\mathcal{O}\). The pair formed by each policy and the corresponding outcome is stored in a buffer defining the population where the search phase will be conducted.&lt;/p&gt;

&lt;p&gt;Concerning the search loop, the following cycle is repeated until convergence. First, a set of outcomes is sampled from the outcome space \(\mathcal{O}\). Second, this sampled outcomes are fed to the search module which looks in the available population for the closest policy parameters that achieve the sampled outcomes (simply using the \(K\)-nearest neighbors algorithm for instance). Third, a noise is applied to the policy parameters picked from the previous step. This promotes behavioral diversity and enables the potential discovery of new outcomes. In fact, the noisy policy parameters are fed to the outcome extractor, yielding an outcome for each entry. Finally, the obtained outcomes are appended to the initial outcome space \(\mathcal{O}\), while the pairs of policy parameters and the corresponding outcomes are added to the initial population.&lt;/p&gt;

&lt;h3 id=&quot;geps-policy-gradient-perspective&quot;&gt;GEPs: Policy Gradient Perspective&lt;/h3&gt;

&lt;p&gt;While the objective of GEPs from the policy search perspective is to maximize the size of the explored population of \(&amp;lt;policy, outcome&amp;gt;\)  pairs, the policy gradient view presents it differently. In this perspective, the output of the process can be a single policy and a set of goals that the policy can achieve. In the policy gradient perspective, the policy is conditioned on the goals. The process leverages two phases: first a &lt;em&gt;bootstrapping phase&lt;/em&gt; to initialize the goal space, then a &lt;em&gt;babbling loop&lt;/em&gt; to learn and discover new goals.&lt;/p&gt;

&lt;p&gt;During the bootstrapping phase, the goal space \(\mathcal{G}\) is filled with either a set of arbitrarily discovered or externally predefined goals, depending on the nature of motivations considered within the process.&lt;/p&gt;

&lt;p&gt;During the babbling loop, the following cycle is repeated until convergence. First, a goal generator is used to sample goals from the goal space \(\mathcal{G}\). Second, a rollout module takes as input the sampled goals, the environment, a goal-conditioned reward function, a goal conditioned policy and noise to produce trajectories. This rollout module can be viewed as running an episode within a simulator using an arbitrary policy with predefined noise. Third, the obtained trajectories are stored in a memory buffer, which feeds an update module responsible for adjusting the goal-conditioned policy so that it maximizes the reward. Finally, the new trajectories are used to extract novel goals discovered during play. These goals are added to the initial goal space.&lt;/p&gt;

&lt;p&gt;In the remainder of the document, we adopt the policy gradient perspective. Depending on the origins of goals obtained in the bootstrapping phase, we consider two sub-families of GEPs: externally and internally motivated.&lt;/p&gt;

&lt;h3 id=&quot;externally-motivated-goal-exploration-processes&quot;&gt;Externally Motivated Goal Exploration Processes&lt;/h3&gt;

&lt;p&gt;Externally Motivated Goal Exploration Processes (EMGEPs) is a sub-family of GEPs where goals are predefined externally. Recall that a goal is a pair of a goal achievement function and a goal embedding. During the bootstrapping phase, an external program defines the goals that will be babbled and the corresponding goal achievement functions. If goals are discrete, then all goals are given. If goals are continuous, then both the support and the goal generator are given. See Figure 2 for an illustration.&lt;/p&gt;

&lt;p&gt;If the goal generation process is embedded within the simulator and not the agent, then the corresponding GEP is considered as an EMGEP. Standard works that tackle the multi-goal reinforcement problem usually define a goal generation function within the environment &lt;a class=&quot;citation&quot; href=&quot;#schaul2015universal&quot;&gt; [4,5,9,10]&lt;/a&gt;. If goals are given by an external program, such as an external artificial or human agent, the corresponding GEP is also considered as an EMGEP. In particular, instruction following agents are the most straightforward EMGEPs, where agents are fully dependent on external goals in the form of natural language instructions &lt;a class=&quot;citation&quot; href=&quot;#hermann2017grounded&quot;&gt; [11,12,13,14,15,16]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;intrinsically-motivated-goal-exploration-processes&quot;&gt;Intrinsically Motivated Goal Exploration Processes&lt;/h3&gt;

&lt;p&gt;Intrinsically Motivated Goal Exploration Processes (IMGEPs) is a sub-family of GEPs where goals are exclusively discovered by the exploring agents itself. In other words, there is no external signal to provide goal embeddings nor goal achievement functions. Initially, during the bootstrapping phase, IMGEP agents have no clue whatsoever on the goal space. They use an arbitrary policy performing random actions in the environment and unlocks easy goals that are close in distribution term to the distributions of initial states. Once a sufficient set of goals is discovered, the babbling phase kicks off. As opposed to the first phase, the babbling phase uses a goal-conditioned policy. The exploration-exploitation dilemma is stronger in IMGEPs: the exploration should be efficient enough to avoid getting stuck in a particular distribution of discovered goals, but should be smooth enough to avoid catastrophic forgetting or getting the policy stuck in a local minimum.&lt;/p&gt;

&lt;p&gt;For IMGEPs, the goal generation process is inherent to the agent. It is the agent itself that discovers the goals that it learns about (that is, it discovers both goal embeddings and goal achievement functions). Note that IMGEPs can discover a goal space whose support is defined externally (example: 3D positions, relational predicatess) &lt;a class=&quot;citation&quot; href=&quot;#nair2018visual&quot;&gt; [17,6,18,7,19]&lt;/a&gt;, or a goal space that is previously learned in an unsupervised fashion, using information theory techniques for example &lt;a class=&quot;citation&quot; href=&quot;#warde2018unsupervised&quot;&gt; [20]&lt;/a&gt;, see Figure 2 for an illustration.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/types_gep.jpg&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.2-Illustration of the two sub-families of Goal Exploration Processes (GEPs): (left) IMGEPs (right) EMGEPs. Each type has its own bootstrapping phase but both share the same babbling loop.
&lt;/p&gt;

&lt;h2 id=&quot;autotelic-reinforcement-learning&quot;&gt;Autotelic Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;The term &lt;em&gt;autotelic&lt;/em&gt; was first introduced by the humanistic psychologist Mihaly Csikszenmihaly as part of his theory of &lt;em&gt;flow&lt;/em&gt;. The latter corresponds to a mental state within which embodied agents are deeply involved in some complex activity without external rewarding signals &lt;a class=&quot;citation&quot; href=&quot;#mihaly2000beyond&quot;&gt; [21]&lt;/a&gt;. His observations was based on studying painters, rock climbers and other persons who show full enjoyment in the process of their activity without direct compensation. He refers at these activites as ``autotelic”, which implies that the motivating purposes (&lt;em&gt;telos&lt;/em&gt;) come from the embodied agents themselves (&lt;em&gt;auto&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;In Artificial Intelligence, the term is used to define artificial agents that are self-motivated, self-organized and self-developing &lt;a class=&quot;citation&quot; href=&quot;#steels2004autotelic&quot;&gt; [22,23]&lt;/a&gt;. More formally, autotelic agents are &lt;em&gt;intrinsically motivated&lt;/em&gt; to represent, generate, pursue and learn about their &lt;em&gt;own goals&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#colas2022vygotskian&quot;&gt; [23]&lt;/a&gt;. In the context of goal exploration processes, these agents are IMGEPs endowed with an &lt;em&gt;internal goal generator&lt;/em&gt;: the goals that are explored and learned about depend only on the agents themselves.&lt;/p&gt;

&lt;p&gt;In this section, we present an overview on recent autotelic reinforcement learning—autotelic agents trained with RL algorithms. We distinguish three categories, depending on whether the goal space and the set of reachable goals is known in advance. First, we present the case where autotelic agents do not know the goal space representation, but need to learn it themselves in an unsupervised fashion. Second, we present the case where autotelic agents know the goal space representation beforehand, but have no clue on which goals they can physically reach. Finally, we present the case where autotelic agents know both the goal space representation and the set of reachable goals, but need to self-organize their learning in order to master these goals.&lt;/p&gt;

&lt;h3 id=&quot;autotelic-learning-of-goal-representations&quot;&gt;Autotelic Learning of Goal Representations&lt;/h3&gt;

&lt;p&gt;When the structure of the goal space is not known in advance, artificial agents need to autonomously learn good representations by themselves. They usually rely on &lt;em&gt;information theory&lt;/em&gt; methods which leverage quantities such as entropy measures and mutual information &lt;a class=&quot;citation&quot; href=&quot;#eysenbach2018diversity&quot;&gt; [24,25]&lt;/a&gt;. The main idea is to efficiently explore their state space and extract interesting features that enable them to discover new skills, which they attempt to master afterwards. They use generative models such as variational auto-encoders &lt;a class=&quot;citation&quot; href=&quot;#kingma2019introduction&quot;&gt; [26]&lt;/a&gt; to embed high-dimensional states into compact latent codes &lt;a class=&quot;citation&quot; href=&quot;#laversanne2018curiosity&quot;&gt; [27,17,28]&lt;/a&gt;. The underlying latent space forms the goal space, and generating a latent vector from these generative models corresponds to generating a goal from the goal space. While these approaches are task-agnostic, they usually do not leverage a sufficiently high level of abstraction. In fact, since states are usually continuous, distinguishing two different high level features corresponding to two close states is challenging (e.g. distinguishing when two blocks are close to each other without further information). Besides, the learned goal representation is usually tied to the training-set distribution, and thus cannot generate well to new situations.&lt;/p&gt;

&lt;h3 id=&quot;autotelic-discovery-of-goals&quot;&gt;Autotelic Discovery of Goals&lt;/h3&gt;

&lt;p&gt;When artificial agents know the structure of the goal space but have no clue about the goals that can be physically reached within this space, they need to efficiently explore and discover skills by themselves &lt;a class=&quot;citation&quot; href=&quot;#ecoffet2019go&quot;&gt; [29,30,18,7,19]&lt;/a&gt;. Such scenarios become more challenging if randomly generated goals are likely to be physically unfeasible &lt;a class=&quot;citation&quot; href=&quot;#akakzia2021grounding&quot;&gt; [7,19]&lt;/a&gt;. In this case, the only goals that the agents can learn about are the ones that they have discovered through random exploration. Consequently, such agents need to have efficient exploration mechanisms that overcome bottlenecks and explore sparsely visited regions of their goal space. They might also need additional features such as the ability to imagine new goals based on previous ones &lt;a class=&quot;citation&quot; href=&quot;#colas2020language&quot;&gt; [18]&lt;/a&gt;, or to start exploring from specific states that maximize the discovery of new goals &lt;a class=&quot;citation&quot; href=&quot;#ecoffet2019go&quot;&gt; [29,30,7]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;autotelic-mastery-of-goals&quot;&gt;Autotelic Mastery of Goals&lt;/h3&gt;

&lt;p&gt;In some scenarios, artificial agents can know the structure of their goal space as well as the set of goals they can physically achieve. In other words, any goal they sample using their goal generator can potentially be reached and mastered. The main challenge for these agents is not to discover new goals, but rather to &lt;em&gt;autonomously organize their training goals&lt;/em&gt; in order to master as many skills as possible. This is actually challenging, especially in environments where goals are of different complexities &lt;a class=&quot;citation&quot; href=&quot;#lopes2012strategic&quot;&gt; [31,32,33,6,9,10,7]&lt;/a&gt;. Such agents usually use &lt;em&gt;Automatic Curriculum Learning&lt;/em&gt; (ACL) methods, which rely on proxies such as learning progress or novelty to generate efficient learning curricula &lt;a class=&quot;citation&quot; href=&quot;#lopes2012strategic&quot;&gt; [31,32,33,6,7]&lt;/a&gt;. Besides, other works train generative adversarial networks to produce goals of intermediate difficulty &lt;a class=&quot;citation&quot; href=&quot;#florensa2017reverse&quot;&gt; [34]&lt;/a&gt;, or use methods such as asymmetric self-play to train an adversarial goal generation policy with RL which samples interesting goals for the training agent &lt;a class=&quot;citation&quot; href=&quot;#sukhbaatar2017intrinsic&quot;&gt; [35]&lt;/a&gt;.&lt;/p&gt;

&lt;!--- References --&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;kaelbling1993learning&quot;&gt;[1]L. P. Kaelbling, &lt;i&gt;Learning to Achieve Goals&lt;/i&gt;, in &lt;i&gt;IJCAI&lt;/i&gt; (1993), pp. 1094–1099.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;baranes2013active&quot;&gt;[2]A. Baranes and P.-Y. Oudeyer, &lt;i&gt;Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots&lt;/i&gt;, Robotics and Autonomous Systems &lt;b&gt;61&lt;/b&gt;, 49 (2013).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;forestier2016modular&quot;&gt;[3]S. Forestier and P.-Y. Oudeyer, &lt;i&gt;Modular Active Curiosity-Driven Discovery of Tool Use&lt;/i&gt;, in &lt;i&gt;2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)&lt;/i&gt; (IEEE, 2016), pp. 3965–3972.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;schaul2015universal&quot;&gt;[4]T. Schaul, D. Horgan, K. Gregor, and D. Silver, &lt;i&gt;Universal Value Function Approximators&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (2015), pp. 1312–1320.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;andrychowicz2017hindsight&quot;&gt;[5]M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, &lt;i&gt;Hindsight Experience Replay&lt;/i&gt;, ArXiv Preprint ArXiv:1707.01495 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;colas2019curious&quot;&gt;[6]C. Colas, P.-Y. Oudeyer, O. Sigaud, P. Fournier, and M. Chetouani, &lt;i&gt;CURIOUS: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2019), pp. 1331–1340.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;akakzia2021grounding&quot;&gt;[7]A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud, &lt;i&gt;Grounding Language to Autonomously-Acquired Skills via Goal Generation&lt;/i&gt;, in &lt;i&gt;9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021&lt;/i&gt; (OpenReview.net, 2021).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;forestier2017intrinsically&quot;&gt;[8]S. Forestier, Y. Mollard, and P.-Y. Oudeyer, &lt;i&gt;Intrinsically Motivated Goal Exploration Processes
with Automatic Curriculum Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1708.02190 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lanier2019curiosity&quot;&gt;[9]J. B. Lanier, S. McAleer, and P. Baldi, &lt;i&gt;Curiosity-Driven Multi-Criteria Hindsight Experience Replay&lt;/i&gt;, CoRR &lt;b&gt;abs/1906.03710&lt;/b&gt;, (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;li2019towards&quot;&gt;[10]R. Li, A. Jabri, T. Darrell, and P. Agrawal, &lt;i&gt;Towards Practical Multi-Object Manipulation Using Relational Reinforcement Learning&lt;/i&gt;, ArXiv Preprint &lt;b&gt;abs/1912.11032&lt;/b&gt;, (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hermann2017grounded&quot;&gt;[11]K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. M. Czarnecki, M. Jaderberg, D. Teplyashin, and others, &lt;i&gt;Grounded Language Learning in a Simulated 3D World&lt;/i&gt;, ArXiv Preprint ArXiv:1706.06551 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bahdanau2018learning&quot;&gt;[12]D. Bahdanau, F. Hill, J. Leike, E. Hughes, A. Hosseini, P. Kohli, and E. Grefenstette, &lt;i&gt;Learning to Understand Goal Specifications by Modelling Reward&lt;/i&gt;, ArXiv Preprint ArXiv:1806.01946 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chan2019actrce&quot;&gt;[13]H. Chan, Y. Wu, J. Kiros, S. Fidler, and J. Ba, &lt;i&gt;ACTRCE: Augmenting Experience via Teacher’s Advice For Multi-Goal Reinforcement Learning&lt;/i&gt;, ArXiv Preprint &lt;b&gt;abs/1902.04546&lt;/b&gt;, (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cideron2019self&quot;&gt;[14]G. Cideron, M. Seurin, F. Strub, and O. Pietquin, &lt;i&gt;Self-Educated Language Agent With Hindsight Experience Replay For Instruction Following&lt;/i&gt;, ArXiv Preprint ArXiv:1910.09451 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jiang2019language&quot;&gt;[15]Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn, &lt;i&gt;Language as an Abstraction for Hierarchical Deep Reinforcement Learning&lt;/i&gt;, in &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; (2019), pp. 9414–9426.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fu2019language&quot;&gt;[16]J. Fu, A. Korattikara, S. Levine, and S. Guadarrama, &lt;i&gt;From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following&lt;/i&gt;, ArXiv Preprint ArXiv:1902.07742 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nair2018visual&quot;&gt;[17]A. V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine, &lt;i&gt;Visual Reinforcement Learning with Imagined Goals&lt;/i&gt;, in &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; (2018), pp. 9191–9200.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;colas2020language&quot;&gt;[18]C. Colas, T. Karch, N. Lair, J.-M. Dussoux, C. Moulin-Frier, P. F. Dominey, and P.-Y. Oudeyer, &lt;i&gt;Language as a Cognitive Tool to Imagine Goals in Curiosity Driven
Exploration&lt;/i&gt;, in &lt;i&gt;Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, Virtual&lt;/i&gt;, edited by H. Larochelle, M. A. Ranzato, R. Hadsell, M.-F. Balcan, and H.-T. Lin (2020).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;akakzia2022learning&quot;&gt;[19]A. Akakzia and O. Sigaud, &lt;i&gt;Learning Object-Centered Autotelic Behaviors with Graph Neural Networks&lt;/i&gt;, ArXiv Preprint ArXiv:2204.05141 (2022).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;warde2018unsupervised&quot;&gt;[20]D. Warde-Farley, T. Van de Wiele, T. Kulkarni, C. Ionescu, S. Hansen, and V. Mnih, &lt;i&gt;Unsupervised Control through Non-Parametric Discriminative Rewards&lt;/i&gt;, ArXiv Preprint ArXiv:1811.11359 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mihaly2000beyond&quot;&gt;[21]C. Mihaly, &lt;i&gt;Beyond Boredom and Anxiety: Experiencing Flow in Work and Play&lt;/i&gt; (Jossey-Bass Publishers, 2000).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;steels2004autotelic&quot;&gt;[22]L. Steels, &lt;i&gt;The Autotelic Principle&lt;/i&gt;, in &lt;i&gt;Embodied Artificial Intelligence&lt;/i&gt; (Springer, 2004), pp. 231–242.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;colas2022vygotskian&quot;&gt;[23]C. Colas, T. Karch, C. Moulin-Frier, and P.-Y. Oudeyer, &lt;i&gt;Vygotskian Autotelic Artificial Intelligence: Language and Culture Internalization for Human-Like AI&lt;/i&gt;, ArXiv Preprint ArXiv:2206.01134 (2022).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;eysenbach2018diversity&quot;&gt;[24]B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine, &lt;i&gt;Diversity Is All You Need: Learning Skills without a Reward Function&lt;/i&gt;, ArXiv Preprint ArXiv:1802.06070 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pong2019skew&quot;&gt;[25]V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine, &lt;i&gt;Skew-Fit: State-Covering Self-Supervised Reinforcement Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1903.03698 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kingma2019introduction&quot;&gt;[26]D. P. Kingma, M. Welling, and others, &lt;i&gt;An Introduction to Variational Autoencoders&lt;/i&gt;, Foundations and Trends&amp;amp;#0174 in Machine Learning &lt;b&gt;12&lt;/b&gt;, 307 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;laversanne2018curiosity&quot;&gt;[27]A. Laversanne-Finot, A. Péré, and P.-Y. Oudeyer, &lt;i&gt;Curiosity Driven Exploration of Learned Disentangled
Goal Spaces&lt;/i&gt;, ArXiv Preprint &lt;b&gt;abs/1807.01521&lt;/b&gt;, (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nair2019contextual&quot;&gt;[28]A. Nair, S. Bahl, A. Khazatsky, V. Pong, G. Berseth, and S. Levine, &lt;i&gt;Contextual Imagined Goals for Self-Supervised Robotic Learning&lt;/i&gt;, ArXiv Preprint &lt;b&gt;abs/1910.11670&lt;/b&gt;, (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ecoffet2019go&quot;&gt;[29]A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune, &lt;i&gt;Go-Explore: a New Approach for Hard-Exploration Problems&lt;/i&gt;, ArXiv Preprint ArXiv:1901.10995 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pitis2020maximum&quot;&gt;[30]S. Pitis, H. Chan, S. Zhao, B. Stadie, and J. Ba, &lt;i&gt;Maximum Entropy Gain Exploration for Long Horizon Multi-Goal Reinforcement Learning&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (PMLR, 2020), pp. 7750–7761.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lopes2012strategic&quot;&gt;[31]M. Lopes and P.-Y. Oudeyer, &lt;i&gt;The Strategic Student Approach for Life-Long
Exploration and Learning&lt;/i&gt;, in &lt;i&gt;IEEE International Conference on Development And
Learning and Epigenetic Robotics&lt;/i&gt; (IEEE, 2012), pp. 1–8.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bellemare2016unifying&quot;&gt;[32]M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, &lt;i&gt;Unifying Count-Based Exploration and Intrinsic Motivation&lt;/i&gt;, Advances in Neural Information Processing Systems &lt;b&gt;29&lt;/b&gt;, 1471 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;burda2018large&quot;&gt;[33]Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros, &lt;i&gt;Large-Scale Study of Curiosity-Driven Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1808.04355 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;florensa2017reverse&quot;&gt;[34]C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel, &lt;i&gt;Reverse Curriculum Generation for Reinforcement Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1707.05300 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sukhbaatar2017intrinsic&quot;&gt;[35]S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus, &lt;i&gt;Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play&lt;/i&gt;, ArXiv Preprint ArXiv:1703.05407 (2017).&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;

</description>
                <pubDate>Wed, 21 Sep 2022 00:00:00 +0200</pubDate>
                <link>http://localhost:4000/goal-exploration-processes</link>
                <guid isPermaLink="true">http://localhost:4000/goal-exploration-processes</guid>
                
                
            </item>
        
            <item>
                <title>The Mandlerian Theory on Prelinguistic Concept Formation</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Toddlers are the best learners we know to exist. From a very young age, they exhibit impressive behaviors when interacting with their surroundings. Interestingly, these capacities are not only dependent on caregivers, but can also come from autonomous interactions with objects. So how can an infant learn concepts on her own based only on sensorimotor interactions with the physical world ? On this matter, research has been mainly divided in two conceptually and factually different perspectives: the &lt;em&gt;empiricist&lt;/em&gt; view, suggesting that knowledge comes primarily from sensory experience, and the &lt;em&gt;nativist&lt;/em&gt; view, stating that certain skills and abilities are hard-wired into the brain at birth.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Many approaches in developmental psychology attempted to reconcile between the nativisit and the empiricist point of view by providing a minimal set of innate prerequired cognitive processing capabilities &lt;a class=&quot;citation&quot; href=&quot;#spelke1994initial&quot;&gt; [1,2,3,4,5]&lt;/a&gt;. &lt;a class=&quot;citation&quot; href=&quot;#spelke1994initial&quot;&gt; [1]&lt;/a&gt; proposes four innate modules to handle physics, psychology, geometry and reasoning about numbers. &lt;a class=&quot;citation&quot; href=&quot;#leslie1994tomm&quot;&gt; [2]&lt;/a&gt; suggests primitive modules that leverage causality, animacy and theory of mind. &lt;a class=&quot;citation&quot; href=&quot;#carey2000origin&quot;&gt; [3]&lt;/a&gt; first emphasizes the need for two innate mechanisms to handle intuitive mechanics and intuitive intentional causality, and a third innate module is added in &lt;a class=&quot;citation&quot; href=&quot;#carey2009our&quot;&gt; [5]&lt;/a&gt; to handle the numerical cognitive system. However, all these works propose a rather static and domain-specific list of  primitive capacities, which do not account for the flexibility of the continuous learning in children.&lt;/p&gt;

&lt;p&gt;Recently, cognitive scientist Jean Mandler has proposed a new mechanism called &lt;strong&gt;Perceptual Meaning Analysis&lt;/strong&gt; (PMA), which relies on a minimal collection of primitive core concepts embedded within infants &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6]&lt;/a&gt;. The Mandlerian view recognizes the empiricist claim that concepts are learned during the humans’ lifetime, but argues that a minimal collection of primitives needs to be considered in order for this learning to kick off. The PMA mechanism transforms perceptual inputs into conceptual outputs, also known in the field of cognitive linguistics as &lt;em&gt;image-schemas&lt;/em&gt;. The main difference that distinguishes PMA from earlier mechanisms is that it analyzes a few kinds of &lt;em&gt;spatiotemporal information&lt;/em&gt; from a huge amount of information delivered by perceptual systems. This makes it a &lt;em&gt;minimalistic&lt;/em&gt; and &lt;em&gt;domain-general&lt;/em&gt; mechanism.&lt;/p&gt;

&lt;p&gt;This blog post is organized as follows. First, we describe the theory of image-schemas from a developmental point of view. Then, we focus on the Mandlerian view on the subject and introduce the PMA mechanism. Finally, we argue that such a mechanism could be beneficial in artificial agents as it helps them categorize their sensory perceptions into semantic categories. Our goal is not to implement image-schema extractors nor PMA mechanisms, but rather to extract some concepts behind these developmental theories that can inspire and constrain our models of how agents may learn to represent their environment.&lt;/p&gt;

&lt;h2 id=&quot;the-image-schema-theory&quot;&gt;The Image Schema Theory&lt;/h2&gt;

&lt;p&gt;To understand the meaning of &lt;em&gt;image-schemas&lt;/em&gt;, we propose to disentangle the two composing terms. We attempt to lay the background of our next sections by defining the following concepts from a cognitive linguistics point of view: a schema, an image, and image-schemas.&lt;/p&gt;

&lt;h3 id=&quot;the-schema-theory&quot;&gt;The Schema Theory&lt;/h3&gt;

&lt;p&gt;The term &lt;em&gt;schema&lt;/em&gt; and its plural &lt;em&gt;schemata&lt;/em&gt; have greek roots associated with the terms “form” or “figure”. In the late eighteenth century, Kantian philosophy was interested in schemas, and introduced them as means to relate percepts to concepts &lt;a class=&quot;citation&quot; href=&quot;#kant1908critique&quot;&gt; [7]&lt;/a&gt;. More precisely, Kant starts from a known example to define a schema:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The empirical conception of a plate is homogeneous with the pure geometrical conception of a circle, in as much as the roundness which is cogitated in the former is intuited in the latter.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kant considers a schema as a double-sided mediating representation, where one side is &lt;em&gt;sensuous&lt;/em&gt; and the other is &lt;em&gt;intellectual&lt;/em&gt;. These representations are &lt;em&gt;fixed templates&lt;/em&gt; superimposed onto perceptions and conceptions to render meaningful representations.&lt;/p&gt;

&lt;p&gt;Inspired by this philosophical point of view, cognitive science recently defined a schema as “a cognitive representation comprising a generalization over perceived similarities among instances of usage” &lt;a class=&quot;citation&quot; href=&quot;#kemmer2000introduction&quot;&gt; [8]&lt;/a&gt;. This term was actually adopted decades earlier by the swiss psychologist Jean Piaget, who specically used the french term “schème” to design a framework used to make sense of raw perceptual information &lt;a class=&quot;citation&quot; href=&quot;#piaget1923langage&quot;&gt; [9]&lt;/a&gt;. The main idea is that a schema defines a redescription of events (representation) that brings together in the same category the ones that share some common traits (similarities) to make behavioral generalization possible in humans.  It can be viewed as a representation that allows humans to quickly organize new situations into categories without much effort. This organization, or &lt;em&gt;schematic processing&lt;/em&gt;, starts at a very young age when toddlers first engage with their surroundings.&lt;/p&gt;

&lt;h3 id=&quot;mental-images&quot;&gt;Mental Images&lt;/h3&gt;

&lt;p&gt;Humans have the capacity the continuously generate mental images of different concepts or events. This generation can be a recall from the memory of past experience, or an imaginary construction based on their understanding of certain concepts. The term &lt;em&gt;image&lt;/em&gt; here should not be taken in its literal meaning. In fact, an image does not have to be exclusively visual, but can involve a perceptual collection of auditory, haptic, motoric, olfactory and gustatory experiences &lt;a class=&quot;citation&quot; href=&quot;#oakley2007image&quot;&gt; [10]&lt;/a&gt;. In any case, images are necessarily grounded in perception, providing abstractions on which an individual could eventually add building blocks to frame new experiences. Let’s try to better understand the concept of image with an example. Let’s consider a detailed mental image of a pile of clothes that you may have left on your bed one morning when you were rushing out to work. This image is \textit{specific} to those particular objects (clothes) and not to any other ones. Although this experience involves a particular set of objects, it still serves as an imaginative base for creating a schematized mental image of a stack of any other types of objects. In other words, the image is specific, but can be used with some other component to generalize the concept it represents.&lt;/p&gt;

&lt;h3 id=&quot;image-schemas&quot;&gt;Image-Schemas&lt;/h3&gt;

&lt;p&gt;It was initially the field of contemporary cognitive linguistics that got interested in combining the notions of schema and image &lt;a class=&quot;citation&quot; href=&quot;#arnheim1997visual&quot;&gt; [11,12,10]&lt;/a&gt;. &lt;em&gt;Image-schemas&lt;/em&gt; can be viewed as mental images of abstract context-agnostic concepts.  Thus, unlike an image, an image-schema is not specific nor fixed as it represents highly preconceptual and primitive patterns that enable reasoning in many contexts. To better understand the nuance, we consider the following example. Think of a blue lego brick being put on a red lego brick. This is an image: it is specific and fixed. The underlying image-schema can be the OBJECT ON OBJECT: it is not specific and not fixed. Hence, we are able to construct flexible and abstract representations from specific images. Other examples of image-schemas PATH-GOAL, ATTRACTION, CENTER, PERIPHERY and LINK &lt;a class=&quot;citation&quot; href=&quot;#geeraerts2006cognitive&quot;&gt; [13]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most works in cognitive linguistics followed the Piagetian behaviorism point of view suggesting that concept formation in infants does not happen until language is acquired &lt;a class=&quot;citation&quot; href=&quot;#bogartz1997interpreting&quot;&gt; [14,15,16]&lt;/a&gt;. However, decades of investigations on preverbal infant cognition emphasized the role of prelinguistic conceptualization in understanding the world &lt;a class=&quot;citation&quot; href=&quot;#leslie1994tomm&quot;&gt; [2,1,17,6]&lt;/a&gt;. Notably, some of these works suggest that language comes later as an enrichment to these prelinguistic conceptualization, rather than being a prerequisite &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6,18]&lt;/a&gt;. Interestingly, the same works argue that these prelinguistic image-schemas are strictly &lt;em&gt;spatial&lt;/em&gt;, allowing preverbal toddlers to build their foundational conceptualization capacities by mapping spatial structure into conceptual structure &lt;a class=&quot;citation&quot; href=&quot;#mandler2014defining&quot;&gt; [18]&lt;/a&gt;. The process of formation of these image-schemas depends on some innate perceptual information responsible for monitoring toddlers attentions, as shown in experiments with 2 and 3 months children &lt;a class=&quot;citation&quot; href=&quot;#baillargeon1986representing&quot;&gt; [19,20]&lt;/a&gt;. Recently, a mechanism was proposed to describe the preverbal formation of spatial image-scheme: the Perceptual Meaning Analysis mechanism (PMA) &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-perceptual-meaning-analysis-mechanism&quot;&gt;The Perceptual Meaning Analysis Mechanism&lt;/h2&gt;

&lt;p&gt;The Perceptual Meaning Analysis (PMA) &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6]&lt;/a&gt; is a general framework that accounts for the conceptual activity in the first years of life. It attempts to reconcile between the empiricist and the nativist views. On the one hand, the PMA mechanism recognizes that the concept formation in infants is learned, thus accounting for the empricist view. On the other hand, it argues that in order for this learning to kick off, a minimalist set of primitives need to be considered. In this section, we introduce three main properties of the PMA mechanism which are not leveraged by earlier approaches, thus making it more promising to handle prelinguistic concept formation in infants.&lt;/p&gt;

&lt;h3 id=&quot;pma-translates-temporal-information-into-an-iconic-spatial-form&quot;&gt;PMA translates Temporal Information into an Iconic Spatial Form&lt;/h3&gt;

&lt;p&gt;In the introduction to this section, we highlighted that PMA considers only a set of primitives. A fair question arises: &lt;em&gt;what exactly are these primitives, and how do they make PMA more promising than other approaches?&lt;/em&gt; The theory of prelinguistic concept formation established by PMA suggests that perception-based representation learning is based on &lt;em&gt;attended information&lt;/em&gt;. In fact, it starts the conceptual system by directing the attention of infants to things moving on paths through space &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6]&lt;/a&gt;. A toddler sees for example the hand of her caregiver moving to grasp a toy. It is at the moment of touching the toy (establishing the ``LINK” as described by Mandler &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6]&lt;/a&gt;) that the attention of the toddler gets focused on the specific perception of a hand touching an object. PMA translates this &lt;em&gt;temporal information&lt;/em&gt; (hand moving towards the toy) to &lt;em&gt;iconic spatial form&lt;/em&gt; (hand far from toy, hand in contact with the toy). Based on these thoughts, what is actually innate within infants is the &lt;em&gt;attention capacity towards temporal changes&lt;/em&gt;, allowing them to distinguish different situations based on the contact. That is why the earliest concepts learned correspond to spatial relations &lt;a class=&quot;citation&quot; href=&quot;#mandler2012spatial&quot;&gt; [6]&lt;/a&gt;. Compared to earlier approaches, PMA provides a domain-general mechanism, as infants may learn concepts in one situation (the example of the caregiver reaching the toy), and generalize it to any other situation including a physical contact between two objects. Figure 1 illustrates the idea behind the PMA module.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/pma_module.jpg&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.1 - Illustration of the PMA module. It takes as input temporal information and translates it into iconic spatial static form. It only requires attention to temporal physical changes.
&lt;/p&gt;

&lt;h3 id=&quot;language-supports-pmas-enrichment&quot;&gt;Language supports PMA’s Enrichment&lt;/h3&gt;

&lt;p&gt;The PMA mechanism comes with a minimalistic collection of primitives that allow the acquisition of spatial image-schemas. However, PMA continues to develop as the infant grows up. Namely, the social situatedness that characterizes the early life of toddlers clearly affects their perceptual analysis. In particular, their confrontation to language, as they hear their caregivers engaging with them, unleashes a growing repertoire of new conceptualizations.&lt;/p&gt;

&lt;p&gt;On the one hand, language enables the subdivision of global concepts. In fact, caregivers provide language descriptions of animate or inanimate objects which direct the toddlers’ attention towards features that were originally neglected within their autonomously generated image-schemas. Experiences with 6 months infants show that they begin to use labels provided by adults to subdivide animals &lt;a class=&quot;citation&quot; href=&quot;#fulkerson2007words&quot;&gt; [21]&lt;/a&gt;. Although the child may be globally familiar with a concept, the consistent use of language-based distinctions from adults further directs the child’s attentive analysis, enabling the discovery of novel properties that were originally overlooked.&lt;/p&gt;

&lt;p&gt;On the other hand, language promotes the expansion of the conceptual system beyond spatial information. Recall that the PMA mechanism is initially strictly spatial. In fact, the PMA mechanism deals better with spatial perceptual information because they are usually structured. However, more unstructured sensory information such as colors, tastes and emotions have no primitives within the PMA mechanism. Although infants experience these unstructured information, there is no evidence, to my knowledge, for their conceptualization before language. Language labels provided by adults provide a symbolic system enabling children to map the unstructured perceived information to discrete categories.&lt;/p&gt;

&lt;h3 id=&quot;pma-supports-language-learning&quot;&gt;PMA supports Language Learning&lt;/h3&gt;

&lt;p&gt;Infants come to the language learning task with a set of image-schemas translating their understanding on abstract concepts involving spatial primitives. Interestingly, grammatical relations within language are also abstract, which suggests that these same image-schemas could also play an important role in providing the relational notions that structure sentences. In fact, research on early language acquisition shows that children rely on notions that can be described in image-schema terms &lt;a class=&quot;citation&quot; href=&quot;#tomasello1992social&quot;&gt; [22]&lt;/a&gt;. More importantly, the first explicit grammatical particles that appear in English-speaking toddlers are mainly prepositions such as &lt;em&gt;in&lt;/em&gt; and &lt;em&gt;on&lt;/em&gt; which respectively express &lt;em&gt;containment&lt;/em&gt; and &lt;em&gt;support&lt;/em&gt;. This perfectly fits with the idea that prelinguistic image-schemas which mainly involve spatial concepts support early language learning. To further reinforce this claim, researchers were interested in other languages that, unlike English, are not prepositional. For instance, some experiences involved Korean, within which containment and support are rather expressed by verbs translating a degree of fitness &lt;a class=&quot;citation&quot; href=&quot;#choi1999early&quot;&gt; [23,24]&lt;/a&gt;. The results showed that Korean infants too begin to acquire the common spatial morphemes of their language at the same age as English infants.&lt;/p&gt;

&lt;p&gt;The claim that image-schemas support learning of languages such as English and Korean is possible because studies with these particular languages show that infants tend to first acquire spatial words (prepositions or verbs describing spatial relations). However, the existing variety of human languages makes the generalization of this claim somehow unsupported. In any case, I do not think that image-schemas are necessarily a prerequisite for language learning, even though they might potentially facilitate it.&lt;/p&gt;

&lt;h2 id=&quot;artificial-intelligence-and-perceptual-meaning-analysis&quot;&gt;Artificial Intelligence and Perceptual Meaning Analysis&lt;/h2&gt;

&lt;p&gt;The field of Artificial Intelligence (AI) confronts two opposing currents: &lt;em&gt;connectionist AI&lt;/em&gt; that aims to learn as much as possible from data in an end-to-end fashion; &lt;em&gt;symbolic AI&lt;/em&gt; which implements inductive biases and several hand-coded symbolic modules. This opposition is analogous to the one between empiricism and nativism described throughout this section (where empiricism is close to connectionist AI and nativism to symbolic AI). We align with the variety of research believing that these two currents are actually complementary. Recently, research has been investigating the middle grounding between symbolic AI and connectionist AI by incorporating symbolic representations within end-to-end computation tools such as neural networks. These approaches, called &lt;em&gt;neuro-symbolic AI&lt;/em&gt;, are shown to be successful in many domains such as control &lt;a class=&quot;citation&quot; href=&quot;#leon2020systematic&quot;&gt; [25]&lt;/a&gt;, visual question answering &lt;a class=&quot;citation&quot; href=&quot;#andreas2016learning&quot;&gt; [26,27]&lt;/a&gt; and theorem proving &lt;a class=&quot;citation&quot; href=&quot;#minervini2018towards&quot;&gt; [28]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As PMA describes a model for prelinguistic concept formation in infants, endowing artificial agents with a similar mechanism seems promising. More specifically, embodied artificial agents that are endowed with raw sensors might make use of a conceptual PMA-like mechanism. Such agents would be able not only to perceive their world as it is, but to build concepts and categorizations based on spatial relations. In Figure 2, we illustrate the potential capabilities of PMA-based agents compared with standard ones. PMA-based agents would in principle be able to categorize their sensory perceptions into semantic categories based on the underlying semantic features. This might facilitate skill acquisition, facilitate language grounding and increase behavioral diversity.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/pma.jpg&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.2 - Embodied Artificial Agents with and without PMA module.
&lt;/p&gt;

&lt;!--- References --&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;spelke1994initial&quot;&gt;[1]E. Spelke, &lt;i&gt;Initial Knowledge: Six Suggestions&lt;/i&gt;, Cognition &lt;b&gt;50&lt;/b&gt;, 431 (1994).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;leslie1994tomm&quot;&gt;[2]A. M. Leslie, &lt;i&gt;ToMM, ToBy, and Agency: Core Architecture and Domain Specificity&lt;/i&gt;, Mapping the Mind: Domain Specificity in Cognition and Culture &lt;b&gt;29&lt;/b&gt;, 119 (1994).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;carey2000origin&quot;&gt;[3]S. Carey, &lt;i&gt;The Origin of Concepts&lt;/i&gt;, Journal of Cognition and Development &lt;b&gt;1&lt;/b&gt;, 37 (2000).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;leslie2005developmental&quot;&gt;[4]A. M. Leslie, &lt;i&gt;Developmental Parallels in Understanding Minds and Bodies&lt;/i&gt;, Trends in Cognitive Sciences &lt;b&gt;9&lt;/b&gt;, 459 (2005).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;carey2009our&quot;&gt;[5]S. Carey, &lt;i&gt;Where Our Number Concepts Come From&lt;/i&gt;, The Journal of Philosophy &lt;b&gt;106&lt;/b&gt;, 220 (2009).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mandler2012spatial&quot;&gt;[6]J. M. Mandler, &lt;i&gt;On the Spatial Foundations of the Conceptual System and Its Enrichment&lt;/i&gt;, Cognitive Science &lt;b&gt;36&lt;/b&gt;, 421 (2012).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kant1908critique&quot;&gt;[7]I. Kant, &lt;i&gt;Critique of Pure Reason. 1781&lt;/i&gt;, Modern Classical Philosophers, Cambridge, MA: Houghton Mifflin 370 (1908).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kemmer2000introduction&quot;&gt;[8]S. Kemmer and M. Barlow, &lt;i&gt;Introduction: A Usage-Based Conception of Language&lt;/i&gt;, Usage-Based Models of Language 7 (2000).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;piaget1923langage&quot;&gt;[9]J. Piaget, &lt;i&gt;Le Langage Et La Pensée Chez l’Enfant&lt;/i&gt;, Vol. 1 (Delachaux and Niestlé, 1923).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;oakley2007image&quot;&gt;[10]T. Oakley, &lt;i&gt;Image Schemas&lt;/i&gt;, The Oxford Handbook of Cognitive Linguistics 214 (2007).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;arnheim1997visual&quot;&gt;[11]R. Arnheim, &lt;i&gt;Visual Thinking&lt;/i&gt; (Univ of California Press, 1997).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;johnson2013body&quot;&gt;[12]M. Johnson, &lt;i&gt;The Body in the Mind: The Bodily Basis of Meaning, Imagination, and Reason&lt;/i&gt; (University of Chicago press, 2013).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;geeraerts2006cognitive&quot;&gt;[13]D. Geeraerts, &lt;i&gt;Cognitive Linguistics: Basic Readings&lt;/i&gt;, Vol. 34 (Walter de Gruyter, 2006).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bogartz1997interpreting&quot;&gt;[14]R. S. Bogartz, J. L. Shinskey, and C. J. Speaker, &lt;i&gt;Interpreting Infant Looking: The Event Set\Times Event Set Design.&lt;/i&gt;, Developmental Psychology &lt;b&gt;33&lt;/b&gt;, 408 (1997).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;haith1998infant&quot;&gt;[15]M. M. Haith and J. B. Benson, &lt;i&gt;Infant Cognition.&lt;/i&gt;, None (1998).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sloutsky2010perceptual&quot;&gt;[16]V. M. Sloutsky, &lt;i&gt;From Perceptual Categories to Concepts: What Develops?&lt;/i&gt;, Cognitive Science &lt;b&gt;34&lt;/b&gt;, 1244 (2010).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mandler1999preverbal&quot;&gt;[17]J. M. Mandler, &lt;i&gt;Preverbal Representation and Language&lt;/i&gt;, Language and Space 365 (1999).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mandler2014defining&quot;&gt;[18]J. M. Mandler and C. P. Cánovas, &lt;i&gt;On Defining Image Schemas&lt;/i&gt;, Language and Cognition &lt;b&gt;6&lt;/b&gt;, 510 (2014).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;baillargeon1986representing&quot;&gt;[19]R. Baillargeon, &lt;i&gt;Representing the Existence and the Location of Hidden Objects: Object Permanence in 6-and 8-Month-Old Infants&lt;/i&gt;, Cognition &lt;b&gt;23&lt;/b&gt;, 21 (1986).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hespos2001reasoning&quot;&gt;[20]S. J. Hespos and R. Baillargeon, &lt;i&gt;Reasoning about Containment Events in Very Young Infants&lt;/i&gt;, Cognition &lt;b&gt;78&lt;/b&gt;, 207 (2001).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fulkerson2007words&quot;&gt;[21]A. L. Fulkerson and S. R. Waxman, &lt;i&gt;Words (but Not Tones) Facilitate Object Categorization: Evidence from 6-and 12-Month-Olds&lt;/i&gt;, Cognition &lt;b&gt;105&lt;/b&gt;, 218 (2007).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tomasello1992social&quot;&gt;[22]M. Tomasello, &lt;i&gt;The Social Bases of Language Acquisition&lt;/i&gt;, Social Development &lt;b&gt;1&lt;/b&gt;, 67 (1992).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;choi1999early&quot;&gt;[23]S. Choi, L. McDonough, M. Bowerman, and J. M. Mandler, &lt;i&gt;Early Sensitivity to Language-Specific Spatial Categories in English and Korean&lt;/i&gt;, Cognitive Development &lt;b&gt;14&lt;/b&gt;, 241 (1999).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mcdonough2003understanding&quot;&gt;[24]L. McDonough, S. Choi, and J. M. Mandler, &lt;i&gt;Understanding Spatial Relations: Flexible Infants, Lexical Adults&lt;/i&gt;, Cognitive Psychology &lt;b&gt;46&lt;/b&gt;, 229 (2003).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;leon2020systematic&quot;&gt;[25]B. G. León, M. Shanahan, and F. Belardinelli, &lt;i&gt;Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning&lt;/i&gt;, ArXiv Preprint ArXiv:2006.08767 (2020).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;andreas2016learning&quot;&gt;[26]J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, &lt;i&gt;Learning to Compose Neural Networks for Question Answering&lt;/i&gt;, ArXiv Preprint ArXiv:1601.01705 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zhu2020overcoming&quot;&gt;[27]X. Zhu, Z. Mao, C. Liu, P. Zhang, B. Wang, and Y. Zhang, &lt;i&gt;Overcoming Language Priors with Self-Supervised Learning for Visual Question Answering&lt;/i&gt;, ArXiv Preprint ArXiv:2012.11528 (2020).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;minervini2018towards&quot;&gt;[28]P. Minervini, M. Bosnjak, T. Rocktäschel, and S. Riedel, &lt;i&gt;Towards Neural Theorem Proving at Scale&lt;/i&gt;, ArXiv Preprint ArXiv:1807.08204 (2018).&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
                <pubDate>Fri, 19 Aug 2022 00:00:00 +0200</pubDate>
                <link>http://localhost:4000/the-mandlerian-theory</link>
                <guid isPermaLink="true">http://localhost:4000/the-mandlerian-theory</guid>
                
                
            </item>
        
            <item>
                <title>How to Motivate Embodied Machines?</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Reinforcement learning represents a mathematical framework for training embodied machines to fulfill a particular task. This task is usually pre-defined by the conceptor, as is modelled as a Markov Decision Process whose reward function usually dictates the learned behavior. However, can these embodied machines learn by their own without this external rewarding signal ? In this blog post, we present a vulgarization of the idea of intrinsically motivated embodied machines.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://theconversation.com/comment-motiver-une-ia-148869&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Continue reading →&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
</description>
                <pubDate>Fri, 20 Nov 2020 00:00:00 +0100</pubDate>
                <link>http://localhost:4000/how-to-motivate-ai</link>
                <guid isPermaLink="true">http://localhost:4000/how-to-motivate-ai</guid>
                
                
            </item>
        
            <item>
                <title>Comparing Multi-task and Meta Reinforcement Learning</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;When tasks and goals are not known in advance, an agent may use either multitask learning or meta reinforcement learning to learn how to transfer knowledge from what it learned before. Recently, goal-conditioned policies and hindsight experience replay have become standard tools to address transfer between goals in the multitask learning setting. In this blog post, I show that these tools can also be imported into the meta reinforcement learning when one wants to address transfer between tasks and between goals at the same time. More importantly, I compare the computation gradients in MAML—a state of the art meta learning algorithm— to gradients in classic multi-task learning setups.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;!--- Introduction --&gt;

&lt;p&gt;In an open-ended learning context &lt;a class=&quot;citation&quot; href=&quot;#doncieux2018open&quot;&gt; [1]&lt;/a&gt;, an agent faces a continuous stream of unforeseen tasks along its lifetime and must learn how to accomplish them.
For doing so, two closely related reinforcement learning (&lt;em&gt;RL&lt;/em&gt;) frameworks are available: multitask learning (&lt;em&gt;MTL&lt;/em&gt;) and meta reinforcement learning (&lt;em&gt;MRL&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Multitask learning was first defined in &lt;a class=&quot;citation&quot; href=&quot;#caruana97multitask&quot;&gt; [2]&lt;/a&gt;. The general idea was to train a unique parametric policy to solve a finite set of tasks so that it would finally perform well on all these tasks. Various &lt;em&gt;MTL&lt;/em&gt; frameworks have been defined since then &lt;a class=&quot;citation&quot; href=&quot;#yang2014unified&quot;&gt; [3]&lt;/a&gt;. For instance, an &lt;em&gt;MTL&lt;/em&gt; agent may learn several policies sharing only a subset of their parameters so that &lt;strong&gt;transfer learning&lt;/strong&gt; can occur between these policies &lt;a class=&quot;citation&quot; href=&quot;#taylor2009transfer&quot;&gt; [4]&lt;/a&gt;.
Multitask learning is the matter of an increasing research effort since the advent of powerful &lt;em&gt;RL&lt;/em&gt; methods &lt;a class=&quot;citation&quot; href=&quot;#florensa2018automatic&quot;&gt; [5,6,7]&lt;/a&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/task_goal.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.1 - The Fetch Environment with different tasks and goals.
&lt;/p&gt;

&lt;p&gt;But this effort came with a drift from the multitask to the multigoal context. Actually, the distinction between tasks and goals is not always clear. In this blog post, I will mainly rely on an intuitive notion illustrated in &lt;em&gt;Figure 1&lt;/em&gt; of task as some abstract activity such as &lt;strong&gt;push blocks&lt;/strong&gt; or &lt;strong&gt;stack blocks&lt;/strong&gt;, and we define a goal as some concrete state of the world that an agent may want to achieve given its current task, such as &lt;strong&gt;pick block “A” and place it inside this specific area&lt;/strong&gt;. If we stick to these definition, a lot of work pretending to transfer between tasks actually transfer between goals. Anyways, for multigoal learning, goal-conditioned policies (&lt;em&gt;GC-P&lt;/em&gt;) have emerged as a satisfactory framework to represent a set of policies to address various goals, as it naturally provides some generalization property between these goals. Besides, the use of Hindsight Experience Replay (&lt;em&gt;HER&lt;/em&gt;) &lt;a class=&quot;citation&quot; href=&quot;#andrychowicz2017hindsight&quot;&gt; [8]&lt;/a&gt; has been shown to significantly speed up the process of learning to reach several goals when the reward is sparse.
In this context, works trying to learn several tasks and several goals at the same time are just emerging (see Table 1 below).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/multi_goal_table.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Table 1. Classification of multi-goal approaches (source &lt;a class=&quot;citation&quot; href=&quot;#colas2019curious&quot;&gt; [9]&lt;/a&gt;).
&lt;/p&gt;

&lt;p&gt;Meta reinforcement learning is a broader framework. Generally speaking, it consists in using inductive bias obtained from learning a set of policies, so that new policies for addressing similar unknown tasks can be learned in only a few gradient steps. In this latter process called &lt;em&gt;fine tuning&lt;/em&gt;, policy parameters are tuned specifically to each new task &lt;a class=&quot;citation&quot; href=&quot;#rakelly2019efficient&quot;&gt; [10]&lt;/a&gt;.
The &lt;em&gt;MRL&lt;/em&gt; framework encompasses several different perspectives &lt;a class=&quot;citation&quot; href=&quot;#weng2019metaRL&quot;&gt; [11]&lt;/a&gt;.
One consists in learning a recurrent neural network whose dynamics update the weights of a policy so as to mimic a reinforcement learning process &lt;a class=&quot;citation&quot; href=&quot;#duan2016rl&quot;&gt; [12,13]&lt;/a&gt;. This approach is classified as &lt;em&gt;context-based&lt;/em&gt; meta-learning in &lt;a class=&quot;citation&quot; href=&quot;#rakelly2019efficient&quot;&gt; [10]&lt;/a&gt; as the internal variables of the recurrent network can be seen as latent context variables.
Another perspective is efficient parameter initialization &lt;a class=&quot;citation&quot; href=&quot;#finn2017model&quot;&gt; [14]&lt;/a&gt;, classified as &lt;em&gt;gradient-based&lt;/em&gt; meta-learning. Here, we focus on a family of algorithms encompassing the second perspective which started with Model-Agnostic Meta-Learning &lt;em&gt;MAML&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#finn2017model&quot;&gt; [14]&lt;/a&gt;. A crucial feature of this family of algorithms is that they introduce a specific mechanism to favor transfer between tasks, by looking for initial policy parameters that are close enough to the manifold of optimal policy parameters over all tasks.&lt;/p&gt;

&lt;p&gt;Actually, though in principle &lt;em&gt;MRL&lt;/em&gt; is more meant to address several tasks than several goals in the same tasks, in practice empirical studies often address the same benchmarks which are multigoal rather than multitask &lt;a class=&quot;citation&quot; href=&quot;#finn2017model&quot;&gt; [14,15,10]&lt;/a&gt;. Only a few papers truly transfer knowledge from one task to another &lt;a class=&quot;citation&quot; href=&quot;#zhao2017tensor&quot;&gt; [16,9,17]&lt;/a&gt;, but at least the latter two do not classify themselves as performing &lt;em&gt;MRL&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The common practice in &lt;em&gt;MTL&lt;/em&gt; consists in sampling from all target tasks, which limits its applicability to the open-ended learning context, where some target tasks may not be known in advance.
Besides, transfer between tasks in &lt;em&gt;MTL&lt;/em&gt;, or rather between goals, usually relies on the generalization capability of the underlying function approximator, without any specific mechanism to improve it, resulting in potential brittleness when this approximator is not appropriate.
Nevertheless, when applied to the multigoal setting using &lt;em&gt;GCPs&lt;/em&gt; and &lt;em&gt;HER&lt;/em&gt;, this approach has been shown to provide efficient transfer capabilities.&lt;/p&gt;

&lt;p&gt;By contrast, in &lt;em&gt;MRL&lt;/em&gt;, the test tasks are left apart during training.
To ensure good transfer to these unseen tasks, the approach in &lt;em&gt;MAML&lt;/em&gt; consists in iteratively refining the initial policy parameters so that a new task can be learned through only a few gradient steps. In principle, this latter transfer-oriented mechanism makes &lt;em&gt;MRL&lt;/em&gt; more appropriate for open-ended learning, where the agent cannot train in advance on future unknown tasks.&lt;/p&gt;

&lt;!--- End Introduction --&gt;

&lt;!--- Gradient computation --&gt;
&lt;p&gt;In this section we compare the computation of gradients in &lt;em&gt;MAML&lt;/em&gt; and the \mtl algorithm we used. At first glance, the differences are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;MAML&lt;/em&gt; uses a meta-optimization step, making it necessary to distinguish gradient computation in the inner and outer update.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;MAML&lt;/em&gt; requires performing new rollouts after each update in order to obtain the validation trajectory set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;em&gt;MAML&lt;/em&gt;, the meta-parameters only change at the end of each epoch. This means that back-propagation needs to be performed through the outer and the inner update. Hence, the gradients are always computed with respect to the initial parameters. By contrast, with &lt;em&gt;MTL&lt;/em&gt;, the model parameters change at each update.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the more detailed presentation given below, we are interested in the expression of the gradient at the end of a single epoch. Notations are provided bit by bit as we show the computational details.&lt;/p&gt;

&lt;h2 id=&quot;maml-gradient&quot;&gt;MAML gradient&lt;/h2&gt;

&lt;p&gt;We initialize the model parameters \(\theta_0\) randomly. We sample a batch of \(N\) tasks and we perform \(k\) inner update for each task:&lt;/p&gt;

\[\begin{align*}
    \theta_0^i &amp;amp; = \theta_0\\
    \theta_1^i &amp;amp; = \theta_0^i - \alpha \nabla_{\theta} \mathcal{L} (\theta_0^i, \mathcal{D}^{tr}_i)\\
    \theta_2^i &amp;amp;= \theta_1^i - \alpha \nabla_{\theta} \mathcal{L} (\theta_1^i, \mathcal{D}^{tr}_i)\\
    &amp;amp; ... \\
    \theta_k^i &amp;amp;= \theta_{k-1}^i - \alpha \nabla_{\theta} \mathcal{L} (\theta_{k-1}^i, \mathcal{D}^{tr}_i)\\
\end{align*}\]

&lt;p&gt;Where \(\mathcal{D}^{tr}_i\) corresponds to the trajectories obtained in the task \(i\) using the parameters before the considered update. At the end of the last inner update of the last task in the batch, we obtain a sequence of parameters {\(\theta_k^1, \theta_k^2, \theta_k^3, ..., \theta_k^N\)} for each of the \(N\) tasks. These parameters are used to generate new trajectories \(\mathcal{D}^{val}_i\) for each task \(i\).&lt;/p&gt;

&lt;p&gt;In the outer loop, \maml uses the newly sampled trajectories to update the meta-objective:&lt;/p&gt;

\[\theta \leftarrow \theta - \beta g_{MAML},\]

&lt;p&gt;where \(g_{MAML}\) is computed in &lt;a class=&quot;citation&quot; href=&quot;#wang2016learning&quot;&gt; [13]&lt;/a&gt;. Here we take back the same computations and add summation across all copies of task-specific parameters:&lt;/p&gt;

\[\begin{align*}
    g_{MAML} &amp;amp;= \nabla_{\theta} \sum_{i=1}^N \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) \\
    &amp;amp;= \sum_{i=1}^N \nabla_{\theta} \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) \\
    &amp;amp;= \sum_{i=1}^N \nabla_{\theta_k^i} \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) . (\nabla_{\theta^i_{k-1}} \theta^i_k) ... (\nabla_{\theta^i_0} \theta^i_1) . (\nabla_{\theta} \theta^i_0)\\
    &amp;amp;= \sum_{i=1}^N \nabla_{\theta_k^i} \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) . {\displaystyle \prod_{j=1}^k \nabla_{\theta^i_{j-1}} \theta^i_j}\\
    &amp;amp;= \sum_{i=1}^N \nabla_{\theta_k^i} \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) . {\displaystyle \prod_{j=1}^k \nabla_{\theta^i_{j-1}} (\theta_{j-1}^i - \alpha \nabla_{\theta} \mathcal{L} (\theta_{j-1}^i, \mathcal{D}^{tr}_i))}\\
    &amp;amp;= \sum_{i=1}^N \nabla_{\theta_k^i} \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) . {\displaystyle \prod_{j=1}^k (I - \alpha \nabla_{\theta^i_{j-1}} (\nabla_{\theta} \mathcal{L} (\theta_{j-1}^i, \mathcal{D}^{tr}_i))}\\
\end{align*}\]

&lt;h2 id=&quot;multitask-learning-algorithm-gradient&quot;&gt;Multitask Learning algorithm gradient&lt;/h2&gt;

&lt;p&gt;In &lt;em&gt;MTL&lt;/em&gt;, there is no distinction between an inner and an outer loop.
The model parameters \(\theta_0\) are initialized randomly. In contrast to &lt;em&gt;MAML&lt;/em&gt;, the gradients do not refer to the initial parameters but to the last updated one. We start by sampling a batch of \(N\) tasks. We perform \(k\) updates for each task sequentially:&lt;/p&gt;

\[\begin{align*}
    \theta_0^1 &amp;amp;= \theta_0 \\
    \theta_1^1 &amp;amp;= \theta_0^1 - \alpha \nabla_{\theta_0^1} \mathcal{L} (\theta_0^1, \mathcal{D})\\
    \theta_2^1 &amp;amp;= \theta_1^1 - \alpha \nabla_{\theta_1^1} \mathcal{L} (\theta_1^1, \mathcal{D})\\
    &amp;amp; ... \\
    \theta_k^1 &amp;amp;= \theta_{k-1}^1 - \alpha \nabla_{\theta_{k-1}^1} \mathcal{L} (\theta_{k-1}^1, \mathcal{D})\\
    \theta_0^2 &amp;amp;= \theta_{k}^1 - \alpha \nabla_{\theta_{k}^1} \mathcal{L} (\theta_{k}^1, \mathcal{D})\\
    &amp;amp; ... \\
    &amp;amp; ... \\
    \theta_{k-1}^N &amp;amp;= \theta_{k-2}^N - \alpha \nabla_{\theta_{k-2}^N} \mathcal{L} (\theta_{k-2}^N, \mathcal{D}),\\

\end{align*}\]

&lt;p&gt;where \(\mathcal{D}\) is a shared set of trajectories between all tasks. Note that sharing the same buffer for different tasks comes with some constraints: tasks must share a common MDP structure, onlmy the reward is task-specific. This is the case of the \fetch robotics environments.&lt;/p&gt;

&lt;p&gt;To get a mathematical expression of the \mtl gradient at the end of an epoch, which we note $g_{MTL}$, we use the recursive relation found above. For simplicity of notations, we note the sequence \((\theta_0^1, ..., \theta_k^1, \theta_0^2, ...\theta_{k-1}^2, ..., \theta_0^N, ..., \theta_{k-1}^N)\) = \((\psi_0, ..., \psi_{Nk})\):&lt;/p&gt;

\[\begin{align*}
    \psi_{Nk} &amp;amp;= \psi_{Nk-1} - \alpha \nabla_{\psi_{Nk-1}} \mathcal{L} (\psi_{Nk-1}, \mathcal{D})\\
    &amp;amp;= \psi_{Nk-2} - \alpha \nabla_{\psi_{Nk-2}} \mathcal{L} (\psi_{Nk-2}, \mathcal{D}) - \alpha \nabla_{\psi_{Nk-1}} \mathcal{L} (\psi_{Nk-1}, \mathcal{D})\\ 
    &amp;amp; ... \\
    &amp;amp;= \psi_0 - \alpha \sum_{i=0}^{Nk-1} \nabla_{\psi_i} \mathcal{L} (\psi_i, \mathcal{D})\\
    &amp;amp;= \psi_0 - \alpha g_{MTL}.
\end{align*}\]

&lt;p&gt;Since \(\psi_i = \theta^{i \div k + 1}_{i \% k}\), the overall update after on epoch is:&lt;/p&gt;

\[\theta \leftarrow \theta - \alpha g_{MTL}.\]

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/maml_fomaml.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.2-Diagram showing the path taken during the optimization step. $\mathcal{A}lg_i$ refers to the inner updates taken during task labeled $i$ (image taken from &lt;a class=&quot;citation&quot; href=&quot;#rajeswaran2019meta&quot;&gt; [18]&lt;/a&gt;)
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/images/mtl.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
Fig.3-Diagram showing the path taken by the used multi-task algorithm. Here the computation is done at each step contrarily to what is shown in Fig 2, where computation is performed in the meta-update step. $\mathcal{A}lg_i$ refers to the updates taken during task labeled $i$.
&lt;/p&gt;

&lt;p&gt;Even though optimization-based \mrl and \mtl may seem very related, the gradients they compute are a lot different. The main differences are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During one single epoch, \mtl tends to update the model parameters during each gradient step, leading to more overall updates than optimization-based &lt;em&gt;MRL&lt;/em&gt;. The latter tends to create copies of the meta-parameters, update each copy according to each task and then back-propagate through the inner optimization step to compute the gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Back-propagating through the inner optimization-step leads to second-order gradients derivative computations. The considered loss functions are differentiable almost everywhere. Computing the second-order derivative may be tricky. In fact, back-propagating through the inner-updates may lead to unstable gradients.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
\boxed{
\begin{array}{rcl}
g_{MAML} &amp;amp; = &amp;amp; \sum_{i=1}^N \nabla_{\theta_k^i} \mathcal{L} (\theta_k^i, \mathcal{D}^{val}_i) . {\displaystyle \prod_{j=1}^k (I - \alpha \nabla_{\theta^i_{j-1}} (\nabla_{\theta} \mathcal{L} (\theta_{j-1}^i, \mathcal{D}^{tr}_i))} \\
g_{MTL} &amp;amp; = &amp;amp; \sum_{i=0}^{Nk-1} \nabla_{\psi_i} \mathcal{L} (\psi_i, \mathcal{D})
\end{array}
}
\end{align*}\]

&lt;!--- References --&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;doncieux2018open&quot;&gt;[1]S. Doncieux, D. Filliat, Dı́az-Rodrı́guez Natalia, T. Hospedales, R. Duro, A. Coninx, D. M. Roijers, B. Girard, N. Perrin, and O. Sigaud, &lt;i&gt;Open-Ended Learning: a Conceptual Framework Based On
                   Representational Redescription&lt;/i&gt;, Frontiers in Robotics and AI &lt;b&gt;12&lt;/b&gt;, (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;caruana97multitask&quot;&gt;[2]R. Caruana, &lt;i&gt;Multitask Learning&lt;/i&gt;, Machine Learning &lt;b&gt;28&lt;/b&gt;, 41 (1997).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yang2014unified&quot;&gt;[3]Y. Yang and T. M. Hospedales, &lt;i&gt;A Unified Perspective on Multi-Domain and Multi-Task
                   Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1412.7489 (2014).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;taylor2009transfer&quot;&gt;[4]M. E. Taylor and P. Stone, &lt;i&gt;Transfer Learning for Reinforcement Learning Domains:
                   A Survey&lt;/i&gt;, Journal of Machine Learning Research &lt;b&gt;10&lt;/b&gt;, 1633 (2009).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;florensa2018automatic&quot;&gt;[5]C. Florensa, D. Held, X. Geng, and P. Abbeel, &lt;i&gt;Automatic Goal Generation for Reinforcement Learning Agents&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning&lt;/i&gt; (2018), pp. 1514–1523.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;veeriah2018many&quot;&gt;[6]V. Veeriah, J. Oh, and S. Singh, &lt;i&gt;Many-Goals Reinforcement Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1806.09605 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ghosh2018learning&quot;&gt;[7]D. Ghosh, A. Gupta, and S. Levine, &lt;i&gt;Learning Actionable Representations With
                   Goal-Conditioned Policies&lt;/i&gt;, ArXiv Preprint ArXiv:1811.07819 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;andrychowicz2017hindsight&quot;&gt;[8]M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, &lt;i&gt;Hindsight Experience Replay&lt;/i&gt;, ArXiv Preprint ArXiv:1707.01495 (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;colas2019curious&quot;&gt;[9]C. Colas, P.-Y. Oudeyer, O. Sigaud, P. Fournier, and M. Chetouani, &lt;i&gt;CURIOUS: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning&lt;/i&gt;, in &lt;i&gt;International Conference on Machine Learning (ICML)&lt;/i&gt; (2019), pp. 1331–1340.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rakelly2019efficient&quot;&gt;[10]K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, &lt;i&gt;Efficient off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables&lt;/i&gt;, ArXiv Preprint ArXiv:1903.08254 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;weng2019metaRL&quot;&gt;[11]L. Weng, &lt;i&gt;Meta Reinforcement Learning&lt;/i&gt;, Lilianweng.github.io/Lil-Log (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;duan2016rl&quot;&gt;[12]Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, &lt;i&gt;RL^2: Fast Reinforcement Learning via Slow
                   Reinforcement Learning&lt;/i&gt;, ArXiv Preprint ArXiv:1611.02779 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2016learning&quot;&gt;[13]J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, &lt;i&gt;Learning to Reinforcement Learn&lt;/i&gt;, ArXiv Preprint ArXiv:1611.05763 (2016).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;finn2017model&quot;&gt;[14]C. Finn, P. Abbeel, and S. Levine, &lt;i&gt;Model-Agnostic Meta-Learning for Fast Adaptation Of
                   Deep Networks&lt;/i&gt;, in &lt;i&gt;Proceedings of the 34th International Conference On
                   Machine Learning-Volume 70&lt;/i&gt; (JMLR. org, 2017), pp. 1126–1135.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rothfuss2018promp&quot;&gt;[15]J. Rothfuss, D. Lee, I. Clavera, T. Asfour, and P. Abbeel, &lt;i&gt;Promp: Proximal Meta-Policy Search&lt;/i&gt;, ArXiv Preprint ArXiv:1810.06784 (2018).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zhao2017tensor&quot;&gt;[16]C. Zhao, T. Hospedales, F. Stulp, and O. Sigaud, &lt;i&gt;Tensor-Based Knowledge Transfer across Skill Categories for Robot Control&lt;/i&gt;, in &lt;i&gt;Proceedings IJCAI&lt;/i&gt; (2017).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fournier2019clic_arxiv&quot;&gt;[17]P. Fournier, O. Sigaud, M. Chetouani, and C. Colas, &lt;i&gt;CLIC: Curriculum Learning and Imitation for Feature Control in Non-Rewarding Environments&lt;/i&gt;, ArXiv Preprint ArXiv:1901.09720 (2019).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rajeswaran2019meta&quot;&gt;[18]A. Rajeswaran, C. Finn, S. Kakade, and S. Levine, &lt;i&gt;Meta-Learning with Implicit Gradients&lt;/i&gt;, ArXiv Preprint ArXiv:1909.04630 (2019).&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
                <pubDate>Thu, 23 May 2019 00:00:00 +0200</pubDate>
                <link>http://localhost:4000/gargaml</link>
                <guid isPermaLink="true">http://localhost:4000/gargaml</guid>
                
                
            </item>
        
            <item>
                <title>From CDN to P2P: a Brief Dataset Analysis for Video Streaming</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Recently, peer-to-peer technology has been welcoming many “shifters” that chose to quit content distribution networks. The reason behind this is the self-scalability of P2P systems provided by the principles of communal collaboration and resource sharing in P2P systems. By building a P2P Content Distribution Network (CDN), peers collaborate to distribute the content of under-provisioned websites and to serve queries for large audiences on behalf of the websites. When designing a P2P CDN, the main challenge is to actually maintain an acceptable level of performance in terms of client-perceived latency and hit ratio while minimizing the incurred overhead.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;This is not a straightforward endeavor given that the P2P CDN relies on autonomous and dynamic peers rather than a dedicated infrastructure. Indeed, the distribution of duties and content over peers should take into account their interests in order to give them proper incentives to cooperate. Moreover, the P2P-CDN should adapt to increasing numbers of participants and provide robust algorithms under high levels of churn because these issues have a key impact on performance. Finally, the routing of queries should aim peers close in locality and serve content from close-by providers to achieve short latencies.&lt;/p&gt;

&lt;p&gt;In this notebook, I will be trying to analyze a dataset about peer-to-peer and cdn content retrieval during nine streams. Each row of the dataset corresponds to a user’s informations for a given stream :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The ID of the stream&lt;/li&gt;
  &lt;li&gt;The Internet Service Providor&lt;/li&gt;
  &lt;li&gt;The type of the browser&lt;/li&gt;
  &lt;li&gt;The status : connected or not&lt;/li&gt;
  &lt;li&gt;P2p retrieval&lt;/li&gt;
  &lt;li&gt;Cdn retrieval&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My two objectives are the following :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Exploring the dataset.&lt;/li&gt;
  &lt;li&gt;Giving some recommendations so that to improve the streaming service.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My approach will be as following : first, I will start with an exploratory data analysis, including some feature processing which will be useful for some interesting plots. Then, according to those visualizations and some personal reads, I will be trying to give some recommendation as to improve the streaming service.&lt;/p&gt;

&lt;h2 id=&quot;table-of-contents-&quot;&gt;Table of Contents :&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#un&quot;&gt;Exploratory data analysis.&lt;br /&gt;&lt;br /&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#deux&quot;&gt;Recommendations.&lt;br /&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&quot;un&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory data analysis&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;#stream&lt;/th&gt;
      &lt;th&gt;isp&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;connected&lt;/th&gt;
      &lt;th&gt;p2p&lt;/th&gt;
      &lt;th&gt;cdn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Fro&lt;/td&gt;
      &lt;td&gt;Iron&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;195910.840977&lt;/td&gt;
      &lt;td&gt;109025.960619&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Fro&lt;/td&gt;
      &lt;td&gt;EarthWolf&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;186711.522041&lt;/td&gt;
      &lt;td&gt;113744.856814&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Arange&lt;/td&gt;
      &lt;td&gt;Iron&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;189428.293434&lt;/td&gt;
      &lt;td&gt;115944.246844&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Arange&lt;/td&gt;
      &lt;td&gt;Iron&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;307577.191067&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;BTP&lt;/td&gt;
      &lt;td&gt;EarthWolf&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;207246.640473&lt;/td&gt;
      &lt;td&gt;107010.608093&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;ISP&lt;/strong&gt; : Internet Service Provider&lt;br /&gt;
&lt;strong&gt;P2P&lt;/strong&gt; : (Peer-to-peer) : a distributed application architecture that partitions tasks or workloads between peers. Peers are equally privileged, equipotent participants in the application. &lt;br /&gt;
&lt;strong&gt;CDN&lt;/strong&gt; : (Content diffusion network)  a geographically distributed network of proxy servers and their data centers. &lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtypes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#stream        int64
isp           object
browser       object
connected       bool
p2p          float64
cdn          float64
dtype: object
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isna&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#stream      0
isp          0
browser      0
connected    0
p2p          0
cdn          1
dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cdn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;#stream&lt;/th&gt;
      &lt;th&gt;isp&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;connected&lt;/th&gt;
      &lt;th&gt;p2p&lt;/th&gt;
      &lt;th&gt;cdn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;534953&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;BTP&lt;/td&gt;
      &lt;td&gt;EarthWolf&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;6100.582815&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We note that there one single NaN value in the dataset. We will deal with it later during the preprocessing&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;True     485753
False     49201
Name: connected, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we see that the number of users that are not connected to the p2p is small compared with the number of users that are actually connected. Thereof, we can consider not connected users as “outliers”, or “distrubing” users. This will be further discussed later on.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;#stream&lt;/th&gt;
      &lt;th&gt;connected&lt;/th&gt;
      &lt;th&gt;p2p&lt;/th&gt;
      &lt;th&gt;cdn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;#stream&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;-0.220267&lt;/td&gt;
      &lt;td&gt;0.329506&lt;/td&gt;
      &lt;td&gt;0.233442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;connected&lt;/th&gt;
      &lt;td&gt;-0.220267&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.090586&lt;/td&gt;
      &lt;td&gt;0.086500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;p2p&lt;/th&gt;
      &lt;td&gt;0.329506&lt;/td&gt;
      &lt;td&gt;0.090586&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.304800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;cdn&lt;/th&gt;
      &lt;td&gt;0.233442&lt;/td&gt;
      &lt;td&gt;0.086500&lt;/td&gt;
      &lt;td&gt;0.304800&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;There is a correlation between cdn / p2p and the stream. This is absolutely normal since the amount of retrieval depends on the nature of the video that is currently being broadcasted&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 5 Internet service providors
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Arange           165341
Fro              149789
BTP              130317
Datch Telecam     45655
Olga              43852
Name: isp, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 4 Types of browsers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EarthWolf    283311
Iron         217246
Vectrice      20970
Swamp         13427
Name: browser, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;data_preprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chained_assignment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Dealing with the single NaN value in the cdn value : Replace it with average cdn on rows in which p2p != 0 for corresponsing
    #stream
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Dealing with NaN ...'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cdn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cdn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'#stream'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                                                          &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p2p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Adding two more features : total amount from cdn and p2p + percentage of p2p for each row
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adding features ...'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'total'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p2p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cdn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'% p2p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p2p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'total'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Preprocessing completed succesfully !'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_preprocessed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_preprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dealing with NaN ...
Adding features ...
Preprocessing completed succesfully !
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_preprocessed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;#stream&lt;/th&gt;
      &lt;th&gt;isp&lt;/th&gt;
      &lt;th&gt;browser&lt;/th&gt;
      &lt;th&gt;connected&lt;/th&gt;
      &lt;th&gt;p2p&lt;/th&gt;
      &lt;th&gt;cdn&lt;/th&gt;
      &lt;th&gt;total&lt;/th&gt;
      &lt;th&gt;% p2p&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Fro&lt;/td&gt;
      &lt;td&gt;Iron&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;195910.840977&lt;/td&gt;
      &lt;td&gt;109025.960619&lt;/td&gt;
      &lt;td&gt;304936.801596&lt;/td&gt;
      &lt;td&gt;64.246375&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Fro&lt;/td&gt;
      &lt;td&gt;EarthWolf&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;186711.522041&lt;/td&gt;
      &lt;td&gt;113744.856814&lt;/td&gt;
      &lt;td&gt;300456.378855&lt;/td&gt;
      &lt;td&gt;62.142639&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Arange&lt;/td&gt;
      &lt;td&gt;Iron&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;189428.293434&lt;/td&gt;
      &lt;td&gt;115944.246844&lt;/td&gt;
      &lt;td&gt;305372.540278&lt;/td&gt;
      &lt;td&gt;62.031869&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Arange&lt;/td&gt;
      &lt;td&gt;Iron&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;307577.191067&lt;/td&gt;
      &lt;td&gt;307577.191067&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;BTP&lt;/td&gt;
      &lt;td&gt;EarthWolf&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;207246.640473&lt;/td&gt;
      &lt;td&gt;107010.608093&lt;/td&gt;
      &lt;td&gt;314257.248566&lt;/td&gt;
      &lt;td&gt;65.948086&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;There are some users that are connected to the p2p but get all the content from cdn (first viewers ? distributed CDN ?)&lt;/li&gt;
  &lt;li&gt;There are users not connected but are part of the p2p = outliers, create noise&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Analysing the not connected users : 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_preprocessed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_preprocessed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'connected'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EarthWolf    24076
Iron         22846
Swamp         1288
Vectrice       991
Name: browser, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Datch Telecam    36415
Fro               7524
Olga              3483
Arange            1641
BTP                138
Name: isp, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar_not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;proportions_isp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;proportions_browser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proportions_browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportions_browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'browser'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'crimson'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proportions_isp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportions_isp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'isp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'darkblue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bar_not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(&amp;lt;Figure size 864x288 with 2 Axes&amp;gt;,
 array([&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001E743692E10&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001E73F5100B8&amp;gt;],
       dtype=object))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_25_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is clear from the left barplot that the different internet service providers do not act the same way in the p2p. For instance, rougly 80% of the “Datch Telecam” users are not connected but are registered in the p2p. In contrast, only a neglectable number of users using “BTP” are registered but not connected. &lt;br /&gt;&lt;br /&gt;
On the other hand, even if the variance is not the same, also browsers do not behave in the same way. Some browsers, more than others, allow users to be registered in the p2p and yet not connected.&lt;br /&gt;&lt;br /&gt;
The fact that a user is not connected but is part of the p2p creates an issue that will be discussed in recommendation part.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
    plots the p2p and cdn for a given stream on a given sliding window
    dataset : the preprocessed dataset
    width : the width of the window
    sn : the stream
    '''&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'#stream'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;values_p2p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;values_cdn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;number_of_batches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;
    
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mean_p2p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'% p2p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;values_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;values_cdn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values_cdn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stackplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p2p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cdn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sandybrown'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dodgerblue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P2p and cdn over a window of {} for stream {}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Percentage'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plot_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_preprocessed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_28_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_28_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_28_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_28_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plot_p2p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_preprocessed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_29_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_29_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_29_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_29_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/p2p_video_streaming_29_4.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The previous 9 subplots, corresponding to the 9 streams present in the dataset, show that percentage of the amount of retrieval from p2p and cdn for each stream. The streams clearly do not behave the same way : for some, the proportion of retrieval from the p2p is comparable and even higher that the amount of retrieval from a cdn. However, for others (for instance stream 3 and 4), the retrieval of the stream from cdn is much more important than that of p2p. Only few users are sharing the stream in the p2p whereas all the others are getting it directly from the cdn.&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;deux&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;recommendation-for-the-service&quot;&gt;Recommendation for the Service&lt;/h2&gt;

&lt;p&gt;The main goal of the streaming providors is to allow the users to watch high-quality videos without any buffering. The P2p architecture was designed to ensure this goal : all the users play the double role of servers and clients. In this scheme, a user can get the streaming from another user which is near him geographically. &lt;strong&gt;This highly reduces the bandwidth cost&lt;/strong&gt; of getting the stream directly from the cdn. However, there are many bottlenecks when using p2p. The exploratory data analysis that I did above shows some issues that need to be tackled in order to provide a better service. The two main issues seen are : &lt;br /&gt;&lt;br /&gt;
     + Many users are registered in the dataset but actually not connected.
     + Many users are watching the same stream, but only few are being in a p2p where the majority is getting the video directly from cdn.
The first issue can be seen as an outlier detection problem : the users that are not connected are “bad” users in a sense that they create noise and disturb the p2p scheme. In fact, in a p2p scheme, some connected users may try to connect to those “not connected” users but will not retrieve anything from them. This is a loss of time and can be costly in term of bandwidth. Therefore, one obvious recommendation would be to detect them and discard them. Nevertheless, we may ask ourselves : how do these outliers emerge ? In fact, there could be many possible anserws. For example, as we saw from the plots, some Internet Service Providors can be bad and not really adapted to the p2p scheme since they register the users in the scheme while they are not actually connected. One can think of contacting these providors to find the actual source of this issue, or even try to adapt the p2p to those providors as well. Before getting to the second issue, I want to point that outlier detection would probably lead to false positive discovery. One ought to reduce the false positives in order not to loose the users &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The second issue concerns the users that are connected, watching the same video as others but not retrieving anything from p2p. Those kind of users naturally exist, since they can be the “first viewers” of the stream and thereof it is normal that there is no one they can retrieve the video from. Yet, when their number grow, this can be considered as an issue : they are not actually benefiting from the p2p. I am not sure, but I think this can be caused by the fact that the circumstances in which they are are not adapted for p2p : there are no other users geographically nearby, or maybe they somehow blocked the p2p retrieval for some reason. An other possible reason is that the stream they are watching is itself blocking a part of the p2p. So an idea can be to contact these stream providors and offer them a different javascript code to adapt with p2p. If the problem is geographical, we can may be think of installing a distrubted network in that area so that to facilicitate the retrieval of the stream.&lt;/p&gt;

&lt;p&gt;This is the first time ever I had the chance to work on a streaming service. I think there are many assets and issues that can be tackled. If well dealt with, the experience of users would be better since he will have access to high quality and fast streaming with no buffering. This is really important since streaming is the new trend for watching videos. &lt;br /&gt;&lt;br /&gt;
I believe that if I had more time, I could have done some other things with this data in order to emphasize on the cited bottlenecks. For example, I would formulate a machine learning problem of outlier detection. I would consider the p2p feature as a target feature and try to fit a function that estimates it from a training sample. Then, I would predict this target variable and see which users have a low / zero p2p retrieval value. These users would be considered as outliers and would be eliminated from the scheme. Then I would redo a plot of the percentage of data retrieval from p2p for the remaining users and see its effect. I think that this percentage will grow and therefor the users will benefit more from the p2p scheme.&lt;/p&gt;
</description>
                <pubDate>Sun, 10 Mar 2019 00:00:00 +0100</pubDate>
                <link>http://localhost:4000/from-cdn-to-p2p</link>
                <guid isPermaLink="true">http://localhost:4000/from-cdn-to-p2p</guid>
                
                
            </item>
        
            <item>
                <title>RAMP on Predictive Maintenance for Aircraft Engines</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Airlines seem to be successful businesses with strong management. Nonetheless, they literally bear high costs due to delays and cancellations that includes expenses on maintenance and compensations to travelers stuck in airports. Besides, they suffer from a non neglectable probability of failing on-flight accidents which can lead to disasters.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;With nearly 30 percent of the total delay time caused by unplanned maintenance and at least 200 accident per year (including fatal accidents) , predictive analytics applied to fleet technical support is a reasonable solution. Predictive maintenance solutions are used to better manage data from aircraft health monitoring sensors. Knowing an aircraft’s current technical condition through alerts, notifications, and reports, employees can spot issues pointing at possible malfunction and replace parts proactively. Executives and team leads, in turn, can receive updates on maintenance operations, get data on tool and part inventory, and expenses via dashboards. With applied predictive maintenance, an airline can reduce expenses connected with expedited transportation of parts, overtime compensation for crews, and unplanned maintenance. If a technical problem did occur, maintenance teams could react to it faster with workflow organization software. The solution consists of analyzing data and metadata regarding detected maintenance activity. It helps engineers quickly evaluate a situation, for instance, to find out if this failure happened for the first time; if not, what can be done to fix it and how much time did it take to solve it previous times.&lt;/p&gt;

&lt;div style=&quot;text-align: left;&quot;&gt;
&lt;table style=&quot;width:100%; background-color:transparent;&quot;&gt;
  &lt;tr style=&quot;background-color:transparent;&quot;&gt;
    &lt;td style=&quot;background-color:transparent;&quot;&gt;&lt;img src=&quot;http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png&quot; width=&quot;70%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt; 
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Salma JERIDI, Aymen DABGHI, Aymen MTIBAA, Ahmed AKAKZIA&lt;/em&gt;&lt;/p&gt;

&lt;div&gt;
&lt;table style=&quot;width:100%; background-color:transparent;&quot;&gt;
  &lt;tr style=&quot;background-color:transparent;&quot;&gt;
    &lt;td style=&quot;background-color:transparent;&quot;&gt;&lt;img src=&quot;https://1l0044267psh26mr7fa57m09-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/opinion.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt; 
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;#un&quot;&gt;1. Prognostics and Health Management&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#deux&quot;&gt;2. Business case&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#deux_un&quot;&gt;2.1 Introduction&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#deux_deux&quot;&gt;2.2 Business Model&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#trois&quot;&gt;3. System and Damage Propoagation Modeling&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#trois_un&quot;&gt;3.1 System Modeling&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#trois_deux&quot;&gt;3.2 Damage Propoagation Modeling&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#quatre&quot;&gt;4. Data for the Challenge&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#cinq&quot;&gt;5. Exploratory Data Analysis&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#six&quot;&gt;6. Features Extraction&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#sept&quot;&gt;7. Classification model&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#huit&quot;&gt;8. Evaluation&lt;br /&gt;&lt;/a&gt;
&lt;a href=&quot;#neuf&quot;&gt;9. Submitting to the online challenge&lt;br /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;un&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-prognostics-and-health-management-phm&quot;&gt;1. Prognostics and Health Management (PHM)&lt;/h1&gt;

&lt;p&gt;Prognostics and health management &lt;em&gt;(PHM)&lt;/em&gt; is an engineering process of failure prevention and predicting reliability and remaining useful lifetime &lt;em&gt;(RUL)&lt;/em&gt;. It has become an important component of many engineering systems and products since it is very crucial to every industry to to detect anomalies, malfunctions and failures before they can damage the whole environment, including the system and the users, and may cause high costs for repairs on un-scheduled maintenance. Inspite of its main objective which is to ensure safety for users, it provides state of the heath of the components and systems which can create a scheduled list to program maintenance before damage. These maintenance tasks become therefore evidence-based scheduled There are two main categories of applications of PHM :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Off-line PHM&lt;/strong&gt; : It concerns systems where safety is not critical and the ratio of failures is very small.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-time PHM&lt;/strong&gt; : It concerns systems where safety is critical and that demand on-board monitoring capability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two approaches that help define the health of a system as the extent of deviation or degradation from its expected typical operating performance which has to be determined accurately to prevent the failures : &lt;strong&gt;Data-driven&lt;/strong&gt; and &lt;strong&gt;Model-driven&lt;/strong&gt;. The former, as its name indicates, is based on data collection via real capturing or simulations based on theoretically-proved models. The latter is rather based on models that describe the system functionalities and specific knowledge.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div&gt;
&lt;table style=&quot;width:100%; background-color:transparent;&quot;&gt;
  &lt;tr style=&quot;background-color:transparent;&quot;&gt;
    &lt;td style=&quot;background-color:transparent;&quot;&gt;&lt;img src=&quot;http://faculty.unist.ac.kr/reliability/wp-content/uploads/sites/357/2017/10/prognostics-and-health.png&quot; width=&quot;70%&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt; 
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;In this challenge&lt;/strong&gt;, we would rather focus on the data-driven approach which uses monitored and historical data to learn the systems behaviours and perform prognostics. It is suitable for systems which are complex and with behaviours that cannot be assessed and deribed from first principles. It uses many algorithms that are quicker to implement and which are computationally more efficient to run compared with other techniques. The data is usually obtained via sensors. One problem of this approach is that the confidence level in the predictions depends on the available historical and empirical data. Besides, it requires some threshold values to be put by the operator which can sometimes be non-trivial.&lt;br /&gt;
Most efforts are focusing on data-driven approaches reflects the desire to harvest low-hanging fruit as compared to model-based approaches. Yet, it can be difficult to gain an access to statistically significant amounts of run-to-failure data and common metrics that allow a comparison between different approaches. Thus, a system model had been established in order to generate run-to failure data that can be utilized to develop, train and test prognostic algorithms. However, before entering into details of system modeling, let’s start by introducing the business case we will be considereing for this challenge.&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;deux&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;2-business-case-&quot;&gt;2. Business Case :&lt;/h1&gt;
&lt;p&gt;&lt;a id=&quot;deux_un&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;21-introduction-&quot;&gt;2.1 Introduction :&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Airlines&lt;/strong&gt; seem to be successful businesses with strong management. Nonetheless, they literally bear high costs due to delays and cancellations that includes expenses on maintenance and compensations to travelers stuck in airports. Besides, they suffer from a non neglectable probability of failing on-flight accidents which can lead to disasters. With nearly 30 percent of the total delay time caused by unplanned maintenance and at least 200 accident per year (including fatal accidents) , predictive analytics applied to fleet technical support is a reasonable solution.&lt;br /&gt;&lt;br /&gt;
Predictive maintenance solutions are used to better manage data from aircraft health monitoring sensors. Knowing an aircraft’s current technical condition through alerts, notifications, and reports, employees can spot issues pointing at possible malfunction and replace parts proactively. Executives and team leads, in turn, can receive updates on maintenance operations, get data on tool and part inventory, and expenses via dashboards.&lt;br /&gt;&lt;br /&gt;
With applied predictive maintenance, an airline can reduce expenses connected with expedited transportation of parts, overtime compensation for crews, and unplanned maintenance. If a technical problem did occur, maintenance teams could react to it faster with workflow organization software. The solution consists of analyzing data and metadata regarding detected maintenance activity. It helps engineers quickly evaluate a situation, for instance, to find out if this failure happened for the first time; if not, what can be done to fix it and how much time did it take to solve it previous times.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Read further :&lt;/strong&gt; https://www.forbes.com/sites/oliverwyman/2017/06/16/the-data-science-revolution-transforming-aviation/#6e2663227f6c&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;deux_deux&quot;&gt; &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;22-business-model-&quot;&gt;2.2 Business Model :&lt;/h2&gt;

&lt;h3 id=&quot;221--business-problem-&quot;&gt;2.2.1 : Business Problem :&lt;/h3&gt;

&lt;p&gt;On the first hand, being ranked second after pilot error, aircraft’s system failure is still one of the five most common reasons for accidents. Equipment failures still account for around 20% of aircraft losses, despite improvements in design and manufacturing quality. While engines are significantly more reliable today than they were half a century ago, they still occasionally suffer catastrophic failures.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Sometimes, new technologies introduce new types of failure. In the 1950s, for example, the introduction of high-flying, pressurised jet aircraft introduced an entirely new hazard – metal fatigue brought on by the hull’s pressurisation cycle. Several high-profile disasters caused by this problem led to the withdrawal of the de Havilland Comet aircraft model, pending design changes.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, if it is discovered that there is a maintenance issue with your aircraft, the flight will not embark until the issue has been fully addressed. Sometimes, these issues are being worked on even as passengers board the plane, meaning the delay the passenger experiences might take place entirely on the tarmac. Other times, in the case of larger issues, your airline might make the call to switch planes entirely for the safety of everyone involved. Thus, this would cost a lot of money for the airline company to handle.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;On-flight accidents and delays cause enormous costs for the airlines each year. Therefore, this issue should be handled.&lt;/p&gt;

&lt;h3 id=&quot;222-our-solution-&quot;&gt;2.2.2: Our solution :&lt;/h3&gt;

&lt;p&gt;First, we have to understand two main concepts: airliners are complex mechanical wonders, and second, their maintenance and operation is very strictly and minutely regulated–and documented. This second point is essential to the aviation regulatory standard upheld by all major airlines, even though such detail must be correctly, diligently accomplished. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Our solution would be to use a data-driven approach in order to schedule the different maintenance tasks in advance. In consists of making use of the different sensors and telemetry available, which depend on the different conditions and modes to predict the order of priority for each engine. In this way, the client would be able to know which engines need to go through maintenance tasks immediately, and which can still wait.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since we talked about the client,  it can be an airline company trying to utilize historical data of equipment sensors to make data-driven decisions on its maintenance planning. Based on this analysis, the company will be able to estimate engine’s order of priority and optimize its maintenance operations accordingly.&lt;/p&gt;

&lt;h3 id=&quot;223--predictive-maintenance-task-&quot;&gt;2.2.3 : Predictive maintenance task :&lt;/h3&gt;
&lt;p&gt;As mentionned previously, reliably estimating remaining life holds the promise for considerable cost savings. In fact, that allows us to avoid unscheduled maintenance and to increase equipment usage, and unsures the operational safety improvements.&lt;/p&gt;

&lt;p&gt;Some studies focus on predicting the Remaining Useful Life (RUL) of the engine of a part and its components, or the exact Time to Failure (TTF). For safety purpose, and considering the error of the predictive model, companies may define a safety threshold ie : a number of cycles that they add to the prediction made by the regressor. This fact makes the alternative of predicting if an asset will fail in different time windows relevant. We formulate then as a Multi-class classification problem and we define 4 four Time to Failure windows (4 classes) :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt; Class 0 : &lt;/strong&gt;Very urgent maintenance : &lt;strong&gt; 0 to 10 cycles &lt;/strong&gt; remaining before failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt; Class 1 : &lt;/strong&gt; Aircraft maintenance periodic checks need to be deep and more detailed : &lt;strong&gt; 11 to 30 cycles &lt;/strong&gt; remaining before failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt; Class 2 : &lt;/strong&gt; Confident system : We can plan from this period the future maintenance date and provide the needed equipments :&lt;strong&gt; 31 to 100  cycles &lt;/strong&gt; remaining before failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt; Class 3: &lt;/strong&gt;Very confident system : Only periodic checks are needed : &lt;strong&gt;more than 101cycles &lt;/strong&gt;remaining before failure.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We set the thresholds based on two key ideas of the predective maintenance : We want to predict the optimal time for maintenance by :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Minimizing the risk of unexpected failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minimizing the cost of early and useless maintenance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;224--business-metrics-&quot;&gt;2.2.4 : Business metrics :&lt;/h3&gt;

&lt;p&gt;In our context, since the key aspect is to avoid failures, it is generally desirable to predict early as compared to predicting late. However, early predictions may also involve significant economic burden. We need then to choose a metric that heavely penalizes late predictions and takes of course into account the early prediction. To do so, we propose to combine two metrics :&lt;/p&gt;

&lt;h4 id=&quot;macro-averaged-f1-score-&quot;&gt;Macro-averaged F1-score :&lt;/h4&gt;

&lt;p&gt;In this score, we need to define the weighted macro precision and recall score. We define the weights w.r.t to the previous condition. In fact,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Assuming we have a One-vs-All (OvA) classifier, we calculate for each class (j) the corresponding precision and recall score :&lt;/li&gt;
&lt;/ol&gt;

\[P_j = \frac{TP_j}{TP_j+FP_j} \quad \quad \quad R_j = \frac{TP_j}{TP_j+FN_j}\]

&lt;ol&gt;
  &lt;li&gt;We average the performances of each individual class using a weight vector. In fact, making a late prediction for an engine having class 0 in the true labels is very dangerous. So, we need to avoid as much as possible the False Negative prediction for elements of the first class. The less is the true maintenance urgency, the more we can tolerate the FN rate. For this reason, we define the weighted Recall rate by :&lt;/li&gt;
&lt;/ol&gt;

\[R = \sum_{i=1}^{4} w_i R_i, \quad \quad \quad w = [0.5, 0.3, 0.1, 0.1]\]

&lt;ol&gt;
  &lt;li&gt;Similarly, falsely predicting that an aircraft is in class 3 is a late prediction and can cause damages specially if the true label is urgent maintenance. We can tolerate a bit more the case of class 2 because in some cases we can have an early prediction, but we still aim to penalize this kind of prediction. To do so, we define the weighted Precision as following :&lt;/li&gt;
&lt;/ol&gt;

\[P = \sum_{i=1}^{4} w_i P_i, \quad \quad \quad w = [0.1, 0.2, 0.3, 0.4]\]

&lt;ol&gt;
  &lt;li&gt;Finally, using the precision &amp;amp; recall calculated previoulsly, we define the macro-averaged F1-score by :&lt;/li&gt;
&lt;/ol&gt;

\[F1 = \frac{2 * P * R}{P+R}\]

&lt;h4 id=&quot;multi-class-logarithmic-loss-function-&quot;&gt;Multi-class logarithmic loss function :&lt;/h4&gt;

&lt;p&gt;We introduce the log loss function for our multicass problem :&lt;/p&gt;

\[LogLoss = − \frac{1}{N}\sum_i^N \sum_j^M y_{ij} \log(p_{ij}))\]

&lt;p&gt;where N is the number of instances, M is the number of different labels, $y_{ij}$ is the binary variable with the expected labels and $p_{ij}$ is the classificiation probability output by the classifier for the i-instance and the j-label.&lt;/p&gt;

&lt;h4 id=&quot;mixed-score&quot;&gt;Mixed score:&lt;/h4&gt;

&lt;p&gt;We define the final score function that we aim to minimize by :&lt;/p&gt;

\[L = LogLoss + (1 - F1_{score})\]

&lt;p&gt;&lt;a id=&quot;trois&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;3-system-and-damage-propagation-modeling-&quot;&gt;3. System and Damage Propagation Modeling :&lt;/h1&gt;

&lt;p&gt;As mentionned in the introduction, data-driven prognostics faces the perennial challenge of the lack of run-to-failure data sets. In most cases realworld, it is time consuming and expensive to have such a data. In other cases, it’s is not even feasible. For example, an airlaine company cannot afford to wait for an engine failure in order to keep the failure exact date. Instead, they perform model/data driven maintenance to avoid the failures.&lt;/p&gt;

&lt;p&gt;In order to face the lack of PHM data, a group of scientist in the U.S. National Aeronautics and Space Administration (NASA) worked on generating run-to-failure data that can then be utilized to develop, train, and test prognostic algorithms. In our challenge, we are going to use this generated data in order to train a robust model that can be used on real sensors data.&lt;/p&gt;

&lt;p&gt;First, let’s introduce the two main concept of data generation.&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;trois_un&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;31-system-modeling&quot;&gt;3.1 System modeling&lt;/h2&gt;

&lt;p&gt;As the goal is to track and to predict the progression of damage in aircraft, the group of scienctist worked first on the simulation of a suitable system model that allows input variations of health related parameters and recording of the resulting output sensor measurements. To do so, they used C-MAPSS (Commercial Modular Aero Propulsion System Simulation) that allow the user to enter specific values of his/her own choice regarding operational profile, closed-loop controllers, environmental conditions, etc, and outputs a simulation of an engine model. A simplified diagram of engine simulated in C-MAPSS and a layout showing various modules and their connections as modeled in the simulation are shown in the following figures :&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Simplified diagram of engine simulated in C-MAPSS&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Various modules and their connections as modeled in the simulation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/akakzia/aircraft_engine_predictive_maintenance/master/image/engine_diagram.PNG&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/akakzia/aircraft_engine_predictive_maintenance/master/image/modules_layout.PNG&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To simulate various degradation scenarios in any of the five rotating components of the simulated engine, C-MAPSS inputs the aircraft configuration. For example, to simulate HPC (High-Pressure Compressor) degradation, C-MAPSS requires the HPC flow and efficiency modifiers. The outputs include various sensor response surfaces. A total of 21 variables out
of 58 different outputs available from the model were used in this challenge.&lt;/p&gt;

&lt;p&gt;As an example of sensor response, we have :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Total temperature at HPC outlet&lt;/li&gt;
  &lt;li&gt;Total temperature at LPT outlet&lt;/li&gt;
  &lt;li&gt;Pressure at fan inlet&lt;/li&gt;
  &lt;li&gt;Total pressure in bypass-duct&lt;/li&gt;
  &lt;li&gt;Total pressure at HPC outlet&lt;/li&gt;
  &lt;li&gt;Physical fan speed&lt;/li&gt;
  &lt;li&gt;Physical core speed, …&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&quot;trois_deux&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;32-damage-propagation-modeling&quot;&gt;3.2 Damage propagation modeling&lt;/h2&gt;

&lt;p&gt;Having decided on the system model, the next step is to model the propagation of damage. The purpose here is to find a model-based method to generate the Time-to-failure (ttf) given the sensors data. In litterature, many degradation models have been proposed. &lt;strong&gt;As an example&lt;/strong&gt;, The Arrhenius model has been used for a variety of failure mechanisms. The operative equation is:
\(t_f = A * e^{\frac{\Delta H}{kT}}\)&lt;/p&gt;

&lt;p&gt;With :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$t_f$ is the time to failure,&lt;/li&gt;
  &lt;li&gt;T is the temperature at the point when the failure process takes place,&lt;/li&gt;
  &lt;li&gt;k is Boltzmann’s constant,&lt;/li&gt;
  &lt;li&gt;A is a scaling factor, and&lt;/li&gt;
  &lt;li&gt;$\Delta H$ is the activation energy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In our case, For the purpose of a physics-inspired data-generation approach, we assume a generalized &lt;strong&gt;equation for wear&lt;/strong&gt; which ignores micro-level processes but retains macro-level degradation characteristics. In addition to this propagation model, two important issues have been considered :&lt;/p&gt;

&lt;h3 id=&quot;321-initial-wear&quot;&gt;3.2.1 Initial Wear&lt;/h3&gt;

&lt;p&gt;Initial wear can occur due to manufacturing inefficiencies and are commonly observed in real systems. In the simulation, the degree of initial wear that an engine might experience with progressive usage was implemented following the calculations made by 
Chatterjee and Litt (see S. Chatterjee and J. Litt, “Online Model Parameter Estimation of Jet Engine Degradation for Autonomous Propulsion Control, “NASA, Technical Manual TM2003-212608, 2003. for more details)&lt;/p&gt;

&lt;h3 id=&quot;322-effects-of-between-flight-maintenance&quot;&gt;3.2.2 Effects of between-flight maintenance&lt;/h3&gt;

&lt;p&gt;The effects of between-flight maintenance have not been explicitly modeled but have been incorporated as the process noise. Since there was no real data available to characterize true noise levels, simplistic normal noise distributions were assumed based on information available from the literature. Finally, To make the signal noise non-trivial, mixture distributions were used.&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;quatre&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-data-for-the-challenge&quot;&gt;4. Data for the challenge&lt;/h1&gt;

&lt;p&gt;During this challenge, we are going to use the NASA’s public data set, result of Engine degradation simulation described previously. Four different sets were simulated under different combinations of operational conditions and fault modes (original data available here : https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/?fbclid=IwAR2ec11zAXo5KH5PcJO0axppSlajScvSrUL17xxbSqUiQ7YpVie31CCSw4s#turbofan)&lt;/p&gt;

&lt;p&gt;For our case, we restrict our study to the combination of two data sets simulated under those conditions :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fault Modes : HPC and/or Fan module Degradation&lt;/li&gt;
  &lt;li&gt;Simulation under sea level condition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Data sets consists of multiple multivariate time series(unit of time is one cycle). Each data set is further divided into training and test subsets. Each time series is from a different engine, i.e, the data can be considered to be from a fleet of engines of the same type.&lt;/p&gt;

&lt;p&gt;The engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. To conclude, the available data is the following :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training Data: The aircraft engine run-to-failure data.&lt;/li&gt;
  &lt;li&gt;Test Data: The aircraft engine operating data without failure events recorded.&lt;/li&gt;
  &lt;li&gt;Ground Truth Data: The true remaining cycles (True TTF) for each engine in the testing data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to fit the data to the problem’s need, we brought the following modifications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;As the training data was not labeled and knowing the system failure cycle, we started by calculating the Time To Failure (TTF) of each observation and then we assigned them to the corresponding classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The true TTF available in the ground truth data correspond to the last observation of each engine(which is not its failure point). So in order to have more data for testing, we used these true TTF to generate the number of remaining cycles for the rest of the observations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;import-libraries&quot;&gt;Import libraries&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;problem&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;get_test_data&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recall_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_curve&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;itertools&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cycle&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_binarize&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;loading-the-data&quot;&gt;Loading the data&lt;/h2&gt;
&lt;p&gt;We start by inspecting the training data&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Load train data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;((45351, 26), (45351,))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table style=&quot;width:10;&quot; border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;Cycle&lt;/th&gt;
      &lt;th&gt;op_set_1&lt;/th&gt;
      &lt;th&gt;op_set_2&lt;/th&gt;
      &lt;th&gt;op_set_3&lt;/th&gt;
      &lt;th&gt;s1&lt;/th&gt;
      &lt;th&gt;s2&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;s15&lt;/th&gt;
      &lt;th&gt;s16&lt;/th&gt;
      &lt;th&gt;s17&lt;/th&gt;
      &lt;th&gt;s18&lt;/th&gt;
      &lt;th&gt;s19&lt;/th&gt;
      &lt;th&gt;s20&lt;/th&gt;
      &lt;th&gt;s21&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;-0.0007&lt;/td&gt;
      &lt;td&gt;-0.0004&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;641.82&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4195&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.06&lt;/td&gt;
      &lt;td&gt;23.4190&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.0019&lt;/td&gt;
      &lt;td&gt;-0.0003&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.15&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4318&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.00&lt;/td&gt;
      &lt;td&gt;23.4236&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;-0.0043&lt;/td&gt;
      &lt;td&gt;0.0003&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.35&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4178&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;390&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.95&lt;/td&gt;
      &lt;td&gt;23.3442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.0007&lt;/td&gt;
      &lt;td&gt;0.0000&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.35&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.3682&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.88&lt;/td&gt;
      &lt;td&gt;23.3739&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;-0.0019&lt;/td&gt;
      &lt;td&gt;-0.0002&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.37&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4294&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;393&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.90&lt;/td&gt;
      &lt;td&gt;23.4044&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 20 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The data consist of 45351 observations and 26 features. 
Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;1) Unit number or ID of the engine ( 200 unique IDs)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2) Time in cycles&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3) to 5) Operational settings 1 to 3&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;6) to 26) Sensor measurements 1 to 21&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These measurements include various sensor response surfaces and operability margins :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;s1 : Total temperature at fan inlet&lt;/li&gt;
  &lt;li&gt;s2: Total temperature at LPC outlet&lt;/li&gt;
  &lt;li&gt;s3 : Total temperature at HPC (High-Pressure Compressor) outlet&lt;/li&gt;
  &lt;li&gt;s4: Total temperature at LPT outlet&lt;/li&gt;
  &lt;li&gt;s5: Pressure at fan inlet&lt;/li&gt;
  &lt;li&gt;s6 : Total pressure in bypass-duct&lt;/li&gt;
  &lt;li&gt;s7 :Total pressure at HPC outlet&lt;/li&gt;
  &lt;li&gt;s8: Physical fan speed&lt;/li&gt;
  &lt;li&gt;s9 :Physical core speed&lt;/li&gt;
  &lt;li&gt;s10: Engine pressure ratio (P50/P2)&lt;/li&gt;
  &lt;li&gt;s11: Static pressure at HPC outlet&lt;/li&gt;
  &lt;li&gt;s12: Ratio of fuel flow to Ps30&lt;/li&gt;
  &lt;li&gt;s13: Corrected fan speed&lt;/li&gt;
  &lt;li&gt;s14: Corrected core speed&lt;/li&gt;
  &lt;li&gt;s15: Bypass Ratio&lt;/li&gt;
  &lt;li&gt;s16: Burner fuel-air ratio&lt;/li&gt;
  &lt;li&gt;s17: Bleed Enthalpy&lt;/li&gt;
  &lt;li&gt;s18: Demanded fan speed&lt;/li&gt;
  &lt;li&gt;s19: Demanded corrected fan speed&lt;/li&gt;
  &lt;li&gt;s20: HPT coolant bleed&lt;/li&gt;
  &lt;li&gt;s21: LPT coolant bleed&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;class 'pandas.core.frame.DataFrame'&amp;gt;
RangeIndex: 45351 entries, 0 to 45350
Data columns (total 26 columns):
ID          45351 non-null int64
Cycle       45351 non-null int64
op_set_1    45351 non-null float64
op_set_2    45351 non-null float64
op_set_3    45351 non-null float64
s1          45351 non-null float64
s2          45351 non-null float64
s3          45351 non-null float64
s4          45351 non-null float64
s5          45351 non-null float64
s6          45351 non-null float64
s7          45351 non-null float64
s8          45351 non-null float64
s9          45351 non-null float64
s10         45351 non-null float64
s11         45351 non-null float64
s12         45351 non-null float64
s13         45351 non-null float64
s14         45351 non-null float64
s15         45351 non-null float64
s16         45351 non-null float64
s17         45351 non-null int64
s18         45351 non-null int64
s19         45351 non-null float64
s20         45351 non-null float64
s21         45351 non-null float64
dtypes: float64(22), int64(4)
memory usage: 9.0 MB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;$\rightarrow$ &lt;strong&gt;There is no missing values to deal with. &lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;Cycle&lt;/th&gt;
      &lt;th&gt;op_set_1&lt;/th&gt;
      &lt;th&gt;op_set_2&lt;/th&gt;
      &lt;th&gt;op_set_3&lt;/th&gt;
      &lt;th&gt;s1&lt;/th&gt;
      &lt;th&gt;s2&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;s18&lt;/th&gt;
      &lt;th&gt;s19&lt;/th&gt;
      &lt;th&gt;s20&lt;/th&gt;
      &lt;th&gt;s21&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;45351&lt;/td&gt;
      &lt;td&gt;45351&lt;/td&gt;
      &lt;td&gt;45351&lt;/td&gt;
      &lt;td&gt;45351&lt;/td&gt;
      &lt;td&gt;45351.0&lt;/td&gt;
      &lt;td&gt;4.535100e+04&lt;/td&gt;
      &lt;td&gt;45351&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;45351.0&lt;/td&gt;
      &lt;td&gt;45351.0&lt;/td&gt;
      &lt;td&gt;45351&lt;/td&gt;
      &lt;td&gt;45351&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;104.447796&lt;/td&gt;
      &lt;td&gt;125.307049&lt;/td&gt;
      &lt;td&gt;-0.000017&lt;/td&gt;
      &lt;td&gt;0.000004&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;5.186700e+02&lt;/td&gt;
      &lt;td&gt;642.559339&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2388.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.910178&lt;/td&gt;
      &lt;td&gt;23.346022&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;56.544862&lt;/td&gt;
      &lt;td&gt;87.813757&lt;/td&gt;
      &lt;td&gt;0.002191&lt;/td&gt;
      &lt;td&gt;0.000294&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;4.637337e-10&lt;/td&gt;
      &lt;td&gt;0.524596&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.236600&lt;/td&gt;
      &lt;td&gt;0.141834&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;-0.008700&lt;/td&gt;
      &lt;td&gt;-0.000600&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;5.186700e+02&lt;/td&gt;
      &lt;td&gt;640.840000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2388.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.140000&lt;/td&gt;
      &lt;td&gt;22.872600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;-0.001500&lt;/td&gt;
      &lt;td&gt;-0.000200&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;5.186700e+02&lt;/td&gt;
      &lt;td&gt;642.180000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2388.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.760000&lt;/td&gt;
      &lt;td&gt;23.254500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;108&lt;/td&gt;
      &lt;td&gt;114&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;5.186700e+02&lt;/td&gt;
      &lt;td&gt;642.520000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2388.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.900000&lt;/td&gt;
      &lt;td&gt;23.342400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;174&lt;/td&gt;
      &lt;td&gt;0.001500&lt;/td&gt;
      &lt;td&gt;0.000300&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;5.186700e+02&lt;/td&gt;
      &lt;td&gt;642.900000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2388.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.050000&lt;/td&gt;
      &lt;td&gt;23.430100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;525&lt;/td&gt;
      &lt;td&gt;0.008700&lt;/td&gt;
      &lt;td&gt;0.000700&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;5.186700e+02&lt;/td&gt;
      &lt;td&gt;645.110000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2388.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.850000&lt;/td&gt;
      &lt;td&gt;23.950500&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;8 rows × 17 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;$\rightarrow$ &lt;strong&gt;The target labels consist of 4 classes representing Time to Failure windows.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# The target labels
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([3, 2, 1, 0])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id=&quot;cinq&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-exploratory-data-analysis-eda&quot;&gt;5. Exploratory Data Analysis (EDA)&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dataCleaning&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    In order to have a proper visualisation, 
    some pre-exploration of the data has been done 
    leading to the following modifications
    &quot;&quot;&quot;&lt;/span&gt; 
       
    &lt;span class=&quot;c1&quot;&gt;# Some features have all their values set to NaN in the correlation matrix, we simply drop them
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s18'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'s19'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'op_set_3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Some feature have 0 correlation with target and the other features, we drop them too
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'op_set_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'op_set_2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s16'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Some features are highly correlated (&amp;gt;0.9), we drop one of them(s9,s14) (s8,s13) 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s12'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s13'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s14'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataCleaning&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;distribution-of-all-engines-with-regards-to-their-failure-cycles&quot;&gt;Distribution of all engines with regards to their failure cycles:&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cycles_per_engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ID'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cycle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cycles_per_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cycles_per_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ID'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Cycles'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cycles_per_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycles&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kde&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Distribution of engines with respect to their failure cycles&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Text(0.5, 1.0, 'Distribution of engines with respect to their failure cycles')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_30_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cycles_per_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Cycles    256.25
Name: 0.75, dtype: float64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;$\rightarrow$ &lt;b&gt;We notice that : 75% of the engines fail before reaching 257 cycles.&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# We will concatenat X_train and y_train for visualisation purposes. 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'labels'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'labels'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;number-of-observations-per-class&quot;&gt;Number of observations per class&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'labels'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Number of observation per class &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Text(0.5, 1.0, 'Number of observation per class ')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_35_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rightarrow$ &lt;b&gt;We notice that : The distribution between classes seems to be imbalanced.&lt;/b&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-correlation-matrix-&quot;&gt;Feature correlation matrix :&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;feature_corr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'.2f'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annot_kws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'size'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'coolwarm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linewidths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_38_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-distributions-&quot;&gt;Feature distributions :&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ft_to_pairplot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s9'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s11'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s17'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'labels'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PairGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ft_to_pairplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;labels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ft_to_pairplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;step&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_upper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kdeplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_40_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rightarrow$ &lt;b&gt;Here we used a  pairplot to see the bivariate relation between each pair of features and distribution of each feature. We notice that:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Classes are not seperated across all feature combinations, there is an overlap in their pairwise relationships.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Almost all features visualized above have normal distributions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relevant-features-inspection-&quot;&gt;Relevant features inspection :&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;viz_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Plot time series of a single engine.
    
        Args:
        eng_id (int64): The id of the engine considered.

    Returns:
        plots
        
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sharex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; Features inspection for engine : &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;## Tempreture at LPC : 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s2(°R)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tempretures at LPC outlet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;## Tempreture at HPC : 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s3(°R)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tempretures at HPC outlet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;## Tempreture at LPT : 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s4(°R)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tempretures at LPT outlet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;## Static pressure at HPC outlet
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s11(psia)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Static pressure at HPC outlet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;## Physical core speed
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s9(rpm)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Physical core speed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;## Physical fan speed
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s8(rpm)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Physical fan speed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;## HPT coolant bleed
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s20(lbm/s)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'HPT coolant bleed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ttf =10'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ttf =30'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ttf =100'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'**************'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;visualize-randomly-chosen-engines-&quot;&gt;Visualize randomly chosen engines :&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Engines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ID'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eng_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Engines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eng&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eng_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;viz_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Features inspection for engine : 54
**************
 Features inspection for engine : 90
**************
 Features inspection for engine : 6
**************
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_45_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_45_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_45_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rightarrow$ &lt;b&gt;It is clear that the behaviour of sensors measurements differ from one class to another and especially when the number of cycles gets close to the failure point. These sensors measurements should then have been more explored in order to extract other valuable and informative information.&lt;/b&gt;&lt;/p&gt;

&lt;h3 id=&quot;classes-caracteristics&quot;&gt;Classes caracteristics:&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;violinplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_conc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;labels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;s2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7fec555c0908&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_48_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rightarrow$ &lt;b&gt;Even though our features’ variances are quite low,these violinplots confirm the previous remark. &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;six&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;6-features-extraction&quot;&gt;6. Features extraction&lt;/h1&gt;

&lt;p&gt;In order to have more informative and insightful data, we are going to generate new features using simple smoothing of the sensors’ measurements.&lt;br /&gt;
Since the observations related to each engine could be considered as time series independent from the other engines’ observations, the above functions will take that into consideration and apply rolling standard deviation (respectively mean) over a defined period of time (number of cycles) of a defined feature of a each engine seperately.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rolling_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cycle_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    For a given dataframe, compute the standard deviation over
    a defined period of time (number of cycles) of a defined feature

    Parameters
    ----------
    data : dataframe
    feature : str
        feature in the dataframe we wish to compute the rolling std from
    cycle_window : str
        string that defines the length of the cycle window passed to rolling
    center : bool
        boolean to indicate if the point of the dataframe considered is
        center or end of the window
    &quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'_'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cycle_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cycle_window&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ffill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rolling_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cycle_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    For a given dataframe, compute the mean over
    a defined period of time (number of cycles) of a defined feature

    Parameters
    ----------
    data : dataframe
    feature : str
        feature in the dataframe we wish to compute the rolling mean from
    cycle_window : str
        string that defines the length of the cycle window passed to rolling
    center : bool
        boolean to indicate if the point of the dataframe considered is
        center or end of the window
    &quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'_'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cycle_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cycle_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;win_type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'hamming'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ffill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_to_return&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id=&quot;sept&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;7-classification-model&quot;&gt;7. Classification model&lt;/h1&gt;

&lt;p&gt;In order to evaluate the performance of the submissions, we provide a test set that can be loaded similarly as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;((29692, 26), (29692,))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;Cycle&lt;/th&gt;
      &lt;th&gt;op_set_1&lt;/th&gt;
      &lt;th&gt;op_set_2&lt;/th&gt;
      &lt;th&gt;op_set_3&lt;/th&gt;
      &lt;th&gt;s1&lt;/th&gt;
      &lt;th&gt;s2&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;s15&lt;/th&gt;
      &lt;th&gt;s16&lt;/th&gt;
      &lt;th&gt;s17&lt;/th&gt;
      &lt;th&gt;s18&lt;/th&gt;
      &lt;th&gt;s19&lt;/th&gt;
      &lt;th&gt;s20&lt;/th&gt;
      &lt;th&gt;s21&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0023&lt;/td&gt;
      &lt;td&gt;0.0003&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;643.02&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4052&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;392&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.86&lt;/td&gt;
      &lt;td&gt;23.3735&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;-0.0027&lt;/td&gt;
      &lt;td&gt;-0.0003&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;641.71&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.3803&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;393&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.02&lt;/td&gt;
      &lt;td&gt;23.3916&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0003&lt;/td&gt;
      &lt;td&gt;0.0001&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.46&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4441&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;393&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.08&lt;/td&gt;
      &lt;td&gt;23.4166&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.0042&lt;/td&gt;
      &lt;td&gt;0.0000&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.44&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.3917&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;391&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;39.00&lt;/td&gt;
      &lt;td&gt;23.3737&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.0014&lt;/td&gt;
      &lt;td&gt;0.0000&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;518.67&lt;/td&gt;
      &lt;td&gt;642.51&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;8.4031&lt;/td&gt;
      &lt;td&gt;0.03&lt;/td&gt;
      &lt;td&gt;390&lt;/td&gt;
      &lt;td&gt;2388&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;38.99&lt;/td&gt;
      &lt;td&gt;23.4130&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 20 columns&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Reload train data without modifications to test the feature extractor
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'./submissions/starting_kit/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;feature_extractor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FeatureExtractor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;classifier&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Classifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_pipeline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FeatureExtractor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([3, 3, 3, ..., 2, 2, 2])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_bin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_binarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id=&quot;huit&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;8-evaluation&quot;&gt;8. Evaluation&lt;/h1&gt;

&lt;h3 id=&quot;sklearn-metrics-&quot;&gt;Sklearn metrics :&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Accuracy :  0.8004176209079887
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[    2,     0,    33,     0],
       [   20,     0,   533,    35],
       [    6,     0,  2728,  3144],
       [    0,     0,  2155, 21036]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;              precision    recall  f1-score   support

           0       0.07      0.06      0.06        35
           1       0.00      0.00      0.00       588
           2       0.50      0.46      0.48      5878
           3       0.87      0.91      0.89     23191

   micro avg       0.80      0.80      0.80     29692
   macro avg       0.36      0.36      0.36     29692
weighted avg       0.78      0.80      0.79     29692
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;roc-curve&quot;&gt;ROC curve&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fpr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tpr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fpr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_curve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_bin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cycle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aqua'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'darkorange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cornflowerblue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fpr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ROC curve of class {0} '&lt;/span&gt;
             &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'False Positive Rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'True Positive Rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ROC-AUC curve for each class with sklearn metrics'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_67_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rightarrow$&lt;strong&gt;High accuracy does not mean a performant model, true and false positives should be taken into cosideration.The above results seem to be good but in reality the classes are imbalanced and have different order of importance (urgency). That’s why we defined our proper metrics that respond to the problem’s needs (weighted metrics). &lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;business-metrics&quot;&gt;Business metrics&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;new_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;c1&quot;&gt;## Multiclass Logloss
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;log_loss : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;## Weighted Precision
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w_prec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Weighted Precision : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_prec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;## Weighted recall : 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;weights_rec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recall_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w_rec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights_rec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Weighted recall : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_rec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;## Macro Average F1
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_rec&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_prec&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m_avg_f1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Macro Average F1 : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_avg_f1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;## Mixed 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;mixed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_avg_f1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Mixed : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;new_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;log_loss :  0.5046380836599733
Weighted Precision :  0.5048226478263285
Weighted recall :  0.16568937431274927
Macro Average F1 :  0.24949216686796474
Mixed :  1.2551459167920087
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_classes_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Plot the probability of a certain class for a single engine.
    
        Args:
        eng_id : The id of the engine considered.
                
    Returns:
        plots
        
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Generate a dataframe having the praobability of predicted classes, the engines IDs, the cycles
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# and the true classes
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;df_y_pred_proba&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Class_0'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Class_1'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Class_2'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Class_3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ID'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ID'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cycle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cycle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y_true'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;

    
    &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Class_0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Class_0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Class_1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Class_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Class_2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Class_2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cycle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Class_3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Class_3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Probability of class for engine : '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Engines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot_classes_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eng_id&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/aircraft_starting_kit_73_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rightarrow$ &lt;b&gt;In the figure above, we clearly notice that we have a problem of a late alarm. In fact, when the true value of the class is 1, our model assigns the higher probability to the class 2 (the segment between the red and green vertical lines). Therefore, we are predicting that our system is confident while it has in reality less than 30 cycles until failure. This can cause an unexpected damage to the aircraft’s engine. &lt;/b&gt;&lt;br /&gt;
$\rightarrow$ &lt;b&gt;We can also be in the case of an early alarm. For example, an engine is a confident system while our model predicts it needs an urgent maintenance. This may involve a useless maintenance and a significant economic burden for the company.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;neuf&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;9-local-testing&quot;&gt;9. Local testing&lt;/h1&gt;

&lt;p&gt;First install ramp-workflow, make sure that the python files feature_extractor.py and classifier.py are in the submission/starting_kit floder then run the following command.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ramp_test_submission&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submission&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;starting_kit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[38;5;178m[1mTesting Predictive maintenance for aircraft engines[0m
[38;5;178m[1mReading train and test files from ./data ...[0m
[38;5;178m[1mReading cv ...[0m
[38;5;178m[1mTraining ./submissions/starting_kit ...[0m
[38;5;178m[1mCV fold 0[0m
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
	[38;5;178m[1mscore  mixed  mc_ll  w_prec  w_rec[0m
	[38;5;10m[1mtrain[0m   [38;5;10m[1m1.13[0m   [38;5;150m0.60[0m    [38;5;150m0.67[0m   [38;5;150m0.36[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.15[0m   [38;5;105m0.55[0m    [38;5;105m0.63[0m   [38;5;105m0.30[0m
	[38;5;1m[1mtest[0m    [38;5;1m[1m1.02[0m   [38;5;218m0.41[0m    [38;5;218m0.66[0m   [38;5;218m0.27[0m
[38;5;178m[1mCV fold 1[0m
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)
	[38;5;178m[1mscore  mixed  mc_ll  w_prec  w_rec[0m
	[38;5;10m[1mtrain[0m   [38;5;10m[1m1.19[0m   [38;5;150m0.60[0m    [38;5;150m0.69[0m   [38;5;150m0.29[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.26[0m   [38;5;105m0.65[0m    [38;5;105m0.64[0m   [38;5;105m0.28[0m
	[38;5;1m[1mtest[0m    [38;5;1m[1m1.09[0m   [38;5;218m0.43[0m    [38;5;218m0.64[0m   [38;5;218m0.24[0m
[38;5;178m[1mCV fold 2[0m
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)
	[38;5;178m[1mscore  mixed  mc_ll  w_prec  w_rec[0m
	[38;5;10m[1mtrain[0m   [38;5;10m[1m1.25[0m   [38;5;150m0.64[0m    [38;5;150m0.62[0m   [38;5;150m0.29[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.21[0m   [38;5;105m0.68[0m    [38;5;105m0.64[0m   [38;5;105m0.38[0m
	[38;5;1m[1mtest[0m    [38;5;1m[1m1.13[0m   [38;5;218m0.47[0m    [38;5;218m0.65[0m   [38;5;218m0.23[0m
[38;5;178m[1mCV fold 3[0m
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)
	[38;5;178m[1mscore  mixed  mc_ll  w_prec  w_rec[0m
	[38;5;10m[1mtrain[0m   [38;5;10m[1m1.12[0m   [38;5;150m0.60[0m    [38;5;150m0.68[0m   [38;5;150m0.37[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.16[0m   [38;5;105m0.61[0m    [38;5;105m0.65[0m   [38;5;105m0.34[0m
	[38;5;1m[1mtest[0m    [38;5;1m[1m1.05[0m   [38;5;218m0.41[0m    [38;5;218m0.64[0m   [38;5;218m0.26[0m
[38;5;178m[1mCV fold 4[0m
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)
	[38;5;178m[1mscore  mixed  mc_ll  w_prec  w_rec[0m
	[38;5;10m[1mtrain[0m   [38;5;10m[1m1.10[0m   [38;5;150m0.59[0m    [38;5;150m0.68[0m   [38;5;150m0.39[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.22[0m   [38;5;105m0.69[0m    [38;5;105m0.65[0m   [38;5;105m0.37[0m
	[38;5;1m[1mtest[0m    [38;5;1m[1m0.97[0m   [38;5;218m0.43[0m    [38;5;218m0.69[0m   [38;5;218m0.34[0m
[38;5;178m[1m----------------------------[0m
[38;5;178m[1mMean CV scores[0m
[38;5;178m[1m----------------------------[0m
	[38;5;178m[1mscore         mixed         mc_ll        w_prec         w_rec[0m
	[38;5;10m[1mtrain[0m  [38;5;10m[1m1.16[0m [38;5;150m[38;5;150m[38;5;150m[38;5;150m±[0m[0m[0m[0m [38;5;150m0.054[0m  [38;5;150m0.61[0m [38;5;150m[38;5;150m[38;5;150m[38;5;150m±[0m[0m[0m[0m [38;5;150m0.018[0m  [38;5;150m0.67[0m [38;5;150m[38;5;150m[38;5;150m[38;5;150m±[0m[0m[0m[0m [38;5;150m0.024[0m  [38;5;150m0.34[0m [38;5;150m[38;5;150m[38;5;150m[38;5;150m±[0m[0m[0m[0m [38;5;150m0.041[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.2[0m [38;5;105m[38;5;105m[38;5;105m[38;5;105m±[0m[0m[0m[0m [38;5;105m0.042[0m  [38;5;105m[38;5;105m0.64[0m[0m [38;5;105m[38;5;105m[38;5;105m[38;5;105m±[0m[0m[0m[0m [38;5;105m0.051[0m  [38;5;105m[38;5;105m0.64[0m[0m [38;5;105m[38;5;105m[38;5;105m[38;5;105m±[0m[0m[0m[0m [38;5;105m0.008[0m  [38;5;105m0.33[0m [38;5;105m[38;5;105m[38;5;105m[38;5;105m±[0m[0m[0m[0m [38;5;105m0.038[0m
	[38;5;1m[1mtest[0m   [38;5;1m[1m1.05[0m [38;5;218m[38;5;218m[38;5;218m[38;5;218m±[0m[0m[0m[0m [38;5;218m0.054[0m  [38;5;218m0.43[0m [38;5;218m[38;5;218m[38;5;218m[38;5;218m±[0m[0m[0m[0m [38;5;218m0.024[0m  [38;5;218m0.66[0m [38;5;218m[38;5;218m[38;5;218m[38;5;218m±[0m[0m[0m[0m [38;5;218m0.016[0m  [38;5;218m0.27[0m [38;5;218m[38;5;218m[38;5;218m[38;5;218m±[0m[0m[0m[0m [38;5;218m0.038[0m
[38;5;178m[1m----------------------------[0m
[38;5;178m[1mBagged scores[0m
[38;5;178m[1m----------------------------[0m
/home/aymen/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
	[38;5;178m[1mscore  mixed[0m
	[38;5;12m[1mvalid[0m   [38;5;12m[1m1.20[0m
	[38;5;1m[1mtest[0m    [38;5;1m[1m1.06[0m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
                <pubDate>Mon, 14 Jan 2019 00:00:00 +0100</pubDate>
                <link>http://localhost:4000/aircraft-starting-kit</link>
                <guid isPermaLink="true">http://localhost:4000/aircraft-starting-kit</guid>
                
                
            </item>
        
    </channel>
</rss>