---
---

References
==========

@article{clavera2018model,
  title={Model-based reinforcement learning via meta-policy optimization},
  author={Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1809.05214},
  year={2018}
}
@article{rothfuss2018promp,
  title={Promp: Proximal meta-policy search},
  author={Rothfuss, Jonas and Lee, Dennis and Clavera, Ignasi and Asfour, Tamim and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1810.06784},
  year={2018}
}
@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993}
  }
@article{zhao2019investigating,
  title={Investigating Generalisation in Continuous Deep Reinforcement Learning},
  author={Zhao, Chenyang and Sigaud, Olivier and Stulp, Freek and Hospedales, Timothy M},
  journal={arXiv preprint arXiv:1902.07015},
  year={2019}
}
@inproceedings{zhao2017tensor,
  title={Tensor-based knowledge transfer across skill categories for robot control},
  author={Zhao, Chenyang and Hospedales, Timothy and Stulp, Freek and Sigaud, Olivier},
  booktitle={Proceedings IJCAI},
    year={2017}
}
@inproceedings{florensa2018automatic,
  title={Automatic Goal Generation for Reinforcement Learning Agents},
  author={Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1514--1523},
  year={2018}
}

@article{veeriah2018many,
  title={Many-Goals Reinforcement Learning},
  author={Veeriah, Vivek and Oh, Junhyuk and Singh, Satinder},
  journal={arXiv preprint arXiv:1806.09605},
  year={2018}
}
@article{fournier2019clic_arxiv,
  title={{CLIC}: Curriculum Learning and Imitation for feature Control in non-rewarding environments},
  author={Fournier, Pierre and Sigaud, Olivier and Chetouani, Mohamed and Colas, C{\'e}dric},
  journal={arXiv preprint arXiv:1901.09720},
  year={2019}
}
@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{haarnoja2018softa,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@inproceedings{colas2019curious,
  title={{CURIOUS}: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning},
  author={Colas, C{\'e}dric and Oudeyer, Pierre-Yves and Sigaud, Olivier and Fournier, Pierre and Chetouani, Mohamed},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1331--1340},
  year={2019}
}

% Added by Ahmed 
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{nichol2018first,
  title={On first-order meta-learning algorithms},
  author={Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{rakelly2019efficient,
  title={Efficient off-policy meta-reinforcement learning via probabilistic context variables},
  author={Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.08254},
  year={2019}
}

@inproceedings{gupta2018meta,
  title={Meta-reinforcement learning of structured exploration strategies},
  author={Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5302--5311},
  year={2018}
}

@article{Wang2016LearningTR,
  title={Learning to reinforcement learn},
  author={Jane X. Wang and Zeb Kurth-Nelson and Dhruva Tirumala and Hubert Soyer and Joel Z. Leibo and R{\'e}mi Munos and Charles Blundell and Dharshan Kumaran and Matthew Botvinick},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.05763}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@article{rajeswaran2019meta,
  title={Meta-Learning with Implicit Gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
  journal={arXiv preprint arXiv:1909.04630},
  year={2019}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{schaul2015universal,
  title={Universal Value Function Approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={1312--1320},
  year={2015}
}

@article{doncieux2018open,
  author =        {Doncieux, Stephane and Filliat, David and
                   D{\'\i}az-Rodr{\'\i}guez, Natalia and
                   Hospedales, Timothy and Duro, Richard and
                   Coninx, Alexandre and Roijers, Diederik M. and
                   Girard, Beno\^{i}t and Perrin, Nicolas and
                   Sigaud, Olivier},
  journal =       {Frontiers in Robotics and AI},
  publisher =     {Frontiers Media SA},
  title =         {Open-ended Learning: a Conceptual Framework based on
                   Representational Redescription},
  volume =        {12},
  year =          {2018},
  doi =           {10.3389/fnbot.2018.00059},
}

@article{caruana97multitask,
  author =        {Caruana, Richard},
  journal =       {Machine Learning},
  number =        {1},
  pages =         {41-75},
  title =         {Multitask Learning},
  volume =        {28},
  year =          {1997},
}

@article{yang2014unified,
  author =        {Yang, Yongxin and Hospedales, Timothy M.},
  journal =       {arXiv preprint arXiv:1412.7489},
  title =         {A unified perspective on multi-domain and multi-task
                   learning},
  year =          {2014},
}

@article{taylor2009transfer,
  author =        {Taylor, Matthew E and Stone, Peter},
  journal =       {Journal of Machine Learning Research},
  number =        {Jul},
  pages =         {1633--1685},
  title =         {Transfer learning for reinforcement learning domains:
                   A survey},
  volume =        {10},
  year =          {2009},
}

@article{andrychowicz2017hindsight,
  author =        {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and
                   Schneider, Jonas and Fong, Rachel and Welinder, Peter and
                   McGrew, Bob and Tobin, Josh and Abbeel, Pieter and
                   Zaremba, Wojciech},
  journal =       {arXiv preprint arXiv:1707.01495},
  title =         {Hindsight {E}xperience {R}eplay},
  year =          {2017},
}

@article{weng2019metaRL,
  author =        {Weng, Lilian},
  journal =       {lilianweng.github.io/lil-log},
  title =         {Meta Reinforcement Learning},
  year =          {2019},
  url =           {http://lilianweng.github.io/lil-log/2019/06/23/meta-
                  reinforcement-learning.html},
}

@inproceedings{finn2017model,
  author =        {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle =     {Proceedings of the 34th International Conference on
                   Machine Learning-Volume 70},
  organization =  {JMLR. org},
  pages =         {1126--1135},
  title =         {Model-agnostic meta-learning for fast adaptation of
                   deep networks},
  year =          {2017},
}

@article{lillicrap2015continuous,
  author =        {Lillicrap, Timothy P and Hunt, Jonathan J and
                   Pritzel, Alexander and Heess, Nicolas and Erez, Tom and
                   Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal =       {arXiv preprint arXiv:1509.02971},
  title =         {Continuous control with deep reinforcement learning},
  year =          {2015},
}

@inproceedings{silver2014deterministic,
  author =        {Silver, David and Lever, Guy and Heess, Nicolas and
                   Degris, Thomas and Wierstra, Daan and
                   Riedmiller, Martin},
  booktitle =     {Proceedings of the 30th International Conference in
                   Machine Learning},
  title =         {Deterministic policy gradient algorithms},
  year =          {2014},
}

@misc{baselines,
  author =        {Dhariwal, Prafulla and Hesse, Christopher and
                   Klimov, Oleg and Nichol, Alex and Plappert, Matthias and
                   Radford, Alec and Schulman, John and Sidor, Szymon and
                   Wu, Yuhuai},
  howpublished =  {\url{https://github.com/openai/baselines}},
  journal =       {GitHub repository},
  publisher =     {GitHub},
  title =         {Open{AI} Baselines},
  year =          {2017},
}

@article{ghosh2018learning,
  author =        {Ghosh, Dibya and Gupta, Abhishek and Levine, Sergey},
  journal =       {arXiv preprint arXiv:1811.07819},
  title =         {Learning Actionable Representations with
                   Goal-Conditioned Policies},
  year =          {2018},
}

@article{duan2016rl,
  author =        {Duan, Yan and Schulman, John and Chen, Xi and
                   Bartlett, Peter L. and Sutskever, Ilya and
                   Abbeel, Pieter},
  journal =       {arXiv preprint arXiv:1611.02779},
  title =         {{RL}$^2$: Fast reinforcement learning via slow
                   reinforcement learning},
  year =          {2016},
}

@article{wang2016learning,
  author =        {Wang, Jane X. and Kurth-Nelson, Zeb and
                   Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and
                   Munos, Remi and Blundell, Charles and
                   Kumaran, Dharshan and Botvinick, Matt},
  journal =       {arXiv preprint arXiv:1611.05763},
  title =         {Learning to reinforcement learn},
  year =          {2016},
}

@article{schulman2015trust,
  author =        {Schulman, John and Levine, Sergey and Moritz, Philipp and
                   Jordan, Michael I. and Abbeel, Pieter},
  journal =       {CoRR, abs/1502.05477},
  title =         {Trust region policy optimization},
  year =          {2015},
}

@article{antoniou2018train,
  author =        {Antoniou, Antreas and Edwards, Harrison and
                   Storkey, Amos},
  journal =       {arXiv preprint arXiv:1810.09502},
  title =         {How to train your {MAML}},
  year =          {2018},
}

@article{plappert2018multi,
  author =        {Plappert, Matthias and Andrychowicz, Marcin and
                   Ray, Alex and McGrew, Bob and Baker, Bowen and
                   Powell, Glenn and Schneider, Jonas and Tobin, Josh and
                   Chociej, Maciek and Welinder, Peter and others},
  journal =       {arXiv preprint arXiv:1802.09464},
  title =         {Multi-Goal Reinforcement Learning: Challenging
                   Robotics Environments and Request for Research},
  year =          {2018},
}

@phdthesis{finn2018learning,
  title={Learning to Learn with Gradients},
  author={Finn, Chelsea},
  year={2018},
  school={UC Berkeley}
}
